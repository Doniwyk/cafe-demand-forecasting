{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Preprocessing\n",
    "\n",
    "This notebook contains the data preprocessing pipeline for the cafe supply forecasting project. It includes:\n",
    "\n",
    "1. Sales Data Merger - Merges sales data from different language sources\n",
    "2. Sales Data Cleaner - Cleans and standardizes sales data\n",
    "3. Raw Materials Preprocessor - Calculates daily raw material requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Optional, List, Set, Tuple\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sales Data Merger\n",
    "\n",
    "This section merges two sales CSV files with different languages (Indonesian and English) into a single standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_indonesian_to_english(df_indonesian: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Translate Indonesian column names and values to English\n",
    "\n",
    "    Args:\n",
    "        df_indonesian: DataFrame with Indonesian column names and values\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with translated English columns and values\n",
    "    \"\"\"\n",
    "    # Column mapping from Indonesian to English\n",
    "    column_mapping = {\n",
    "        'Tanggal': 'Date',\n",
    "        'Nomor struk': 'Receipt number',\n",
    "        'Jenis struk': 'Receipt type',\n",
    "        'Kategori': 'Category',\n",
    "        'SKU': 'SKU',\n",
    "        'Barang': 'Item',\n",
    "        'Varian': 'Variant',\n",
    "        'Pemodifikasi diterapkan': 'Modifiers applied',\n",
    "        'Kuantitas': 'Quantity',\n",
    "        'Penjualan Kotor': 'Gross sales',\n",
    "        'Diskon': 'Discounts',\n",
    "        'Penjualan bersih': 'Net sales',\n",
    "        'Harga pokok': 'Cost of goods',\n",
    "        'Laba kotor': 'Gross profit',\n",
    "        'Pajak': 'Taxes',\n",
    "        'Jenis pesanan': 'Dining option',\n",
    "        'POS': 'POS',\n",
    "        'Toko': 'Store',\n",
    "        'Nama Kasir': 'Cashier name',\n",
    "        'Nama Pelanggan': 'Customer name',\n",
    "        'Kontak Pelanggan': 'Customer contacts',\n",
    "        'Komentar': 'Comment',\n",
    "        'Keadaan': 'Status'\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    df_indonesian = df_indonesian.rename(columns=column_mapping)\n",
    "    \n",
    "    # Translate specific values\n",
    "    value_mappings = {\n",
    "        'Receipt type': {\n",
    "            'Penjualan': 'Sale'\n",
    "        },\n",
    "        'Dining option': {\n",
    "            'Makan di tempat': 'Dine in'\n",
    "        },\n",
    "        'Status': {\n",
    "            'Ditutup': 'Closed'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Apply value translations\n",
    "    for column, mapping in value_mappings.items():\n",
    "        if column in df_indonesian.columns:\n",
    "            df_indonesian[column] = df_indonesian[column].replace(mapping)\n",
    "    \n",
    "    return df_indonesian\n",
    "\n",
    "def parse_date(date_str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Parse different date formats to standard datetime\n",
    "\n",
    "    Args:\n",
    "        date_str: Date string in various formats\n",
    "\n",
    "    Returns:\n",
    "        Parsed datetime object or NaT if parsing fails\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "\n",
    "    date_str = str(date_str).strip()\n",
    "\n",
    "    # Try different date formats\n",
    "    formats_to_try = [\n",
    "        '%d/%m/%y %H.%M',  # Indonesian format: 13/05/25 23.43\n",
    "        '%d/%m/%y %H:%M',  # Alternative Indonesian format\n",
    "        '%m/%d/%y %I:%M %p',  # English format: 9/25/25 10:07 PM\n",
    "        '%m/%d/%Y %I:%M %p',  # English format with 4-digit year\n",
    "    ]\n",
    "\n",
    "    for fmt in formats_to_try:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # If none of the specific formats work, try pandas auto-detection\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def clean_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and convert numeric columns to proper numeric types\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with numeric columns that may contain non-numeric characters\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with cleaned numeric columns\n",
    "    \"\"\"\n",
    "    numeric_columns = [\n",
    "        'Quantity', 'Gross sales', 'Discounts', 'Net sales',\n",
    "        'Cost of goods', 'Gross profit', 'Taxes'\n",
    "    ]\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            # Remove any non-numeric characters except decimal point\n",
    "            df[col] = df[col].astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "            # Convert to numeric, errors='coerce' will turn invalid values to NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_sales_files(file1_path: str, file2_path: str, output_path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Merge two sales CSV files with different languages\n",
    "\n",
    "    Args:\n",
    "        file1_path: Path to Indonesian sales CSV file\n",
    "        file2_path: Path to English sales CSV file\n",
    "        output_path: Path where merged CSV should be saved\n",
    "\n",
    "    Returns:\n",
    "        Merged DataFrame or None if merge fails\n",
    "    \"\"\"\n",
    "    print(f'Reading file 1: {file1_path}')\n",
    "    print(f'Reading file 2: {file2_path}')\n",
    "\n",
    "    # Read both files\n",
    "    try:\n",
    "        # Indonesian file (first file)\n",
    "        df1 = pd.read_csv(file1_path, sep=';')\n",
    "        print(f'File 1 loaded successfully. Shape: {df1.shape}')\n",
    "\n",
    "        # English file (second file)\n",
    "        df2 = pd.read_csv(file2_path, sep=',')\n",
    "        print(f'File 2 loaded successfully. Shape: {df2.shape}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error reading files: {e}')\n",
    "        return None\n",
    "\n",
    "    # Translate Indonesian file to English\n",
    "    print('Translating Indonesian file to English...')\n",
    "    df1_translated = translate_indonesian_to_english(df1)\n",
    "\n",
    "    # Clean numeric columns in both dataframes\n",
    "    print('Cleaning numeric columns...')\n",
    "    df1_translated = clean_numeric_columns(df1_translated)\n",
    "    df2 = clean_numeric_columns(df2)\n",
    "\n",
    "    # Parse dates in both dataframes\n",
    "    print('Parsing dates...')\n",
    "    df1_translated['Date'] = df1_translated['Date'].apply(parse_date)\n",
    "    df2['Date'] = df2['Date'].apply(parse_date)\n",
    "    \n",
    "    # Ensure both dataframes have the same columns\n",
    "    all_columns = set(df1_translated.columns) | set(df2.columns)\n",
    "    \n",
    "    for col in all_columns:\n",
    "        if col not in df1_translated.columns:\n",
    "            df1_translated[col] = np.nan\n",
    "        if col not in df2.columns:\n",
    "            df2[col] = np.nan\n",
    "    \n",
    "    # Reorder columns to match English file order\n",
    "    column_order = [\n",
    "        'Date', 'Receipt number', 'Receipt type', 'Category', 'SKU', 'Item', \n",
    "        'Variant', 'Modifiers applied', 'Quantity', 'Gross sales', 'Discounts', \n",
    "        'Net sales', 'Cost of goods', 'Gross profit', 'Taxes', 'Dining option', \n",
    "        'POS', 'Store', 'Cashier name', 'Customer name', 'Customer contacts', \n",
    "        'Comment', 'Status'\n",
    "    ]\n",
    "    \n",
    "    df1_translated = df1_translated[column_order]\n",
    "    df2 = df2[column_order]\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    print('Combining dataframes...')\n",
    "    combined_df = pd.concat([df1_translated, df2], ignore_index=True)\n",
    "\n",
    "    # Sort by date\n",
    "    print('Sorting by date...')\n",
    "    combined_df = combined_df.sort_values('Date', na_position='last')\n",
    "\n",
    "    # Reset index\n",
    "    combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "    # Save the merged file\n",
    "    print(f'Saving merged file to: {output_path}')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f'Merge completed successfully!')\n",
    "    print(f'Total records: {len(combined_df)}')\n",
    "    print(f'Date range: {combined_df[\"Date\"].min()} to {combined_df[\"Date\"].max()}')\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/raw/sales/receipts-by-item-2022-01-01-2025-06-30.csv\n",
      "Reading file 2: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/raw/sales/receipts-by-item-2025-05-01-2025-09-25.csv\n",
      "File 1 loaded successfully. Shape: (48588, 23)\n",
      "File 2 loaded successfully. Shape: (10189, 23)\n",
      "Translating Indonesian file to English...\n",
      "Cleaning numeric columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/hqyrj5817x1bnh5y864bj0gh0000gp/T/ipykernel_40007/3424459581.py:138: DtypeWarning: Columns (19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1_path, sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dates...\n",
      "Combining dataframes...\n",
      "Sorting by date...\n",
      "Saving merged file to: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data.csv\n",
      "Merge completed successfully!\n",
      "Total records: 58777\n",
      "Date range: 2022-01-01 14:31:00 to 2025-09-25 22:07:00\n",
      "\n",
      "First few rows of merged data:\n",
      "                 Date Receipt number Receipt type Category    SKU  \\\n",
      "0 2022-01-01 14:31:00         Jan-82         Sale      Tea     41   \n",
      "1 2022-01-01 15:34:00         Jan-83         Sale      NaN  10044   \n",
      "2 2022-01-01 15:34:00         Jan-83         Sale      NaN  10049   \n",
      "3 2022-01-01 16:16:00         Jan-84         Sale    snack     84   \n",
      "4 2022-01-01 16:22:00         Jan-85         Sale    Other   3300   \n",
      "\n",
      "                Item  Variant  Modifiers applied  Quantity  Gross sales  ...  \\\n",
      "0      Hot Tea Ajeng      NaN                NaN       1.0      15000.0  ...   \n",
      "1    Rose pink latte      NaN                NaN       1.0      23000.0  ...   \n",
      "2  Roti bakar klasik      NaN                NaN       1.0      10000.0  ...   \n",
      "3     Kentang goreng      NaN                NaN       1.0      15000.0  ...   \n",
      "4          Spaghetti      NaN                NaN       1.0      25000.0  ...   \n",
      "\n",
      "   Gross profit  Taxes  Dining option    POS             Store Cashier name  \\\n",
      "0       13600.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "1       18000.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "2           NaN    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "3       12000.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "4       21000.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "\n",
      "  Customer name Customer contacts Comment  Status  \n",
      "0           NaN               NaN     NaN  Closed  \n",
      "1           NaN               NaN     NaN  Closed  \n",
      "2           NaN               NaN     NaN  Closed  \n",
      "3           NaN               NaN     NaN  Closed  \n",
      "4           NaN               NaN     NaN  Closed  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Data summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58777 entries, 0 to 58776\n",
      "Data columns (total 23 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   Date               58777 non-null  datetime64[ns]\n",
      " 1   Receipt number     58777 non-null  object        \n",
      " 2   Receipt type       58777 non-null  object        \n",
      " 3   Category           52565 non-null  object        \n",
      " 4   SKU                58777 non-null  int64         \n",
      " 5   Item               58777 non-null  object        \n",
      " 6   Variant            0 non-null      float64       \n",
      " 7   Modifiers applied  0 non-null      float64       \n",
      " 8   Quantity           58777 non-null  float64       \n",
      " 9   Gross sales        57419 non-null  float64       \n",
      " 10  Discounts          58750 non-null  float64       \n",
      " 11  Net sales          57409 non-null  float64       \n",
      " 12  Cost of goods      16029 non-null  float64       \n",
      " 13  Gross profit       51141 non-null  float64       \n",
      " 14  Taxes              58777 non-null  float64       \n",
      " 15  Dining option      58763 non-null  object        \n",
      " 16  POS                58777 non-null  object        \n",
      " 17  Store              58777 non-null  object        \n",
      " 18  Cashier name       58777 non-null  object        \n",
      " 19  Customer name      1918 non-null   object        \n",
      " 20  Customer contacts  1876 non-null   object        \n",
      " 21  Comment            986 non-null    object        \n",
      " 22  Status             58777 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(12)\n",
      "memory usage: 10.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Execute the sales data merger\n",
    "base_path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "file1_path = os.path.join(base_path, 'data', 'raw', 'sales', 'receipts-by-item-2022-01-01-2025-06-30.csv')\n",
    "file2_path = os.path.join(base_path, 'data', 'raw', 'sales', 'receipts-by-item-2025-05-01-2025-09-25.csv')\n",
    "output_path = os.path.join(base_path, 'data', 'processed', 'sales_data.csv')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Merge the files\n",
    "merged_data = merge_sales_files(file1_path, file2_path, output_path)\n",
    "\n",
    "if merged_data is not None:\n",
    "    print('\\nFirst few rows of merged data:')\n",
    "    print(merged_data.head())\n",
    "\n",
    "    print('\\nData summary:')\n",
    "    print(merged_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sales Data Cleaner\n",
    "\n",
    "This section cleans and standardizes sales data to match the menu BOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesDataCleaner:\n",
    "    \"\"\"Clean and standardize sales data based on menu BOM\"\"\"\n",
    "\n",
    "    def __init__(self, sales_path: str, menu_bom_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the cleaner with file paths\n",
    "\n",
    "        Args:\n",
    "            sales_path: Path to sales data CSV\n",
    "            menu_bom_path: Path to menu BOM CSV\n",
    "        \"\"\"\n",
    "        self.sales_path = sales_path\n",
    "        self.menu_bom_path = menu_bom_path\n",
    "        self.sales_df = pd.read_csv(sales_path)\n",
    "        self.menu_bom_df = pd.read_csv(menu_bom_path)\n",
    "\n",
    "        # Build mappings\n",
    "        self.rename_map = self._build_rename_mapping()\n",
    "        self.package_items = self._build_package_mapping()\n",
    "        self.active_items = self._get_active_items()\n",
    "\n",
    "        # Track statistics\n",
    "        self.stats = {\n",
    "            'original_records': len(self.sales_df),\n",
    "            'renamed_records': 0,\n",
    "            'expanded_packages': 0,\n",
    "            'expanded_items': 0,\n",
    "            'discontinued_items': 0,\n",
    "            'removed_records': 0\n",
    "        }\n",
    "\n",
    "    def _build_rename_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Build mapping of old names to standardized names\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping old item names to new standardized names\n",
    "        \"\"\"\n",
    "        # Define rename mappings (sales name → BOM name)\n",
    "        rename_map = {\n",
    "            # Tea items\n",
    "            'lemon tea hot': 'Lemon Hot',\n",
    "            'lemon tea ice': 'Lemon Ice',\n",
    "            'hot tea ajeng': 'Ajeng Hot',\n",
    "            'ice tea ajeng': 'Ajeng Ice',\n",
    "            'hot tea anindya': 'Anindya Hot',\n",
    "            'ice tea anindya': 'Anindya Ice',\n",
    "\n",
    "            # Coffee items\n",
    "            'kopi susu panas': 'Kopi Susu Hot',\n",
    "            'ice cappucino': 'Cappucino Ice',\n",
    "            'latte': 'Latte Hot',\n",
    "            'ice latte': 'Latte Ice',\n",
    "            'picolo': 'Piccolo',\n",
    "            'long black hot': 'Black Hot',\n",
    "            'long black ice': 'Black Ice',\n",
    "\n",
    "            # Milk-based items\n",
    "            'red velvet': 'Red Velvet Ice',\n",
    "            'susu coklat': 'Coklat Hot',\n",
    "\n",
    "            # V60 variants (all consolidate to v60)\n",
    "            'v60 hacienda natural': 'v60',\n",
    "            'v60 argopuro': 'v60',\n",
    "            'v60 finca': 'v60',\n",
    "\n",
    "            # Noodle items\n",
    "            'mie goreng telur': 'Mie Goreng',\n",
    "            'mie rebus telur': 'Mie Rebus',\n",
    "\n",
    "            # Rice items\n",
    "            'nasi goreng djawa': 'Nasi Goreng Jawa',\n",
    "            'nasi ayam daun jeruk': 'Ayam Daun Jeruk',\n",
    "            'nasi ayam rempah': 'Ayam Rempah',\n",
    "\n",
    "            # Main course items\n",
    "            'ayam mentega (paket)': 'Ayam Mentega',\n",
    "            'ayam curry (paket)': 'Ayam Curry',\n",
    "            'ayam dauh jeruk (paket)': 'Ayam Daun Jeruk',\n",
    "\n",
    "            # Snacks\n",
    "            'cireng isi': 'Cireng',\n",
    "\n",
    "            # Desserts\n",
    "            'pannacotta': 'Panacotta',\n",
    "\n",
    "            # Additional items\n",
    "            'add vanilla ice cream': 'Add Ice Cream Vanilla',\n",
    "            'add telur ceplok': 'Add Telur',\n",
    "            'add telur dadar': 'Add Telur',\n",
    "            'add caramel topping': 'Add Topping Caramel',\n",
    "        }\n",
    "\n",
    "        return rename_map\n",
    "\n",
    "    def _build_package_mapping(self) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"\n",
    "        Build mapping for package deals that need to be split\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping package names to list of (item, quantity multiplier) tuples\n",
    "        \"\"\"\n",
    "        package_map = {\n",
    "            # Mie with telur - add telur as separate item\n",
    "            'mie goreng telur': [\n",
    "                ('Mie Goreng', 1.0),\n",
    "                ('Add Telur', 1.0)\n",
    "            ],\n",
    "            'mie rebus telur': [\n",
    "                ('Mie Rebus', 1.0),\n",
    "                ('Add Telur', 1.0)\n",
    "            ],\n",
    "            # Package deals\n",
    "            'paket ayam teriyaki + lemon tea': [\n",
    "                ('Ayam Teriyaki', 1.0),\n",
    "                ('Lemon Ice', 1.0)\n",
    "            ],\n",
    "            'paket ayam curry + lemon tea': [\n",
    "                ('Ayam Curry', 1.0),\n",
    "                ('Lemon Ice', 1.0)\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        return package_map\n",
    "\n",
    "    def _get_active_items(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get set of active items from menu BOM\n",
    "\n",
    "        Returns:\n",
    "            Set of normalized active item names\n",
    "        \"\"\"\n",
    "        active_items = set()\n",
    "\n",
    "        # Get unique items from menu BOM\n",
    "        for item_name in self.menu_bom_df['Item'].unique():\n",
    "            # Normalize by stripping whitespace and converting to lowercase\n",
    "            normalized = item_name.strip().lower()\n",
    "            active_items.add(normalized)\n",
    "\n",
    "        return active_items\n",
    "\n",
    "    def _normalize_item_name(self, item_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize item name for comparison\n",
    "\n",
    "        Args:\n",
    "            item_name: Original item name\n",
    "\n",
    "        Returns:\n",
    "            Normalized item name (lowercase, stripped)\n",
    "        \"\"\"\n",
    "        if pd.isna(item_name):\n",
    "            return ''\n",
    "        return str(item_name).strip().lower()\n",
    "\n",
    "    def standardize_names(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Standardize item names in sales data\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to standardize\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with standardized item names\n",
    "        \"\"\"\n",
    "        print('\\n' + '-' * 80)\n",
    "        print('STEP 1: STANDARDIZING ITEM NAMES')\n",
    "        print('-' * 80)\n",
    "\n",
    "        # Create a copy to work with\n",
    "        standardized_df = df.copy()\n",
    "\n",
    "        # Apply simple renames first\n",
    "        renamed_count = 0\n",
    "        for old_name, new_name in self.rename_map.items():\n",
    "            # Case-insensitive matching\n",
    "            mask = standardized_df['Item'].str.lower() == old_name.lower()\n",
    "            if mask.any():\n",
    "                count = mask.sum()\n",
    "                standardized_df.loc[mask, 'Item'] = new_name\n",
    "                renamed_count += count\n",
    "                print(f'  ✓ Renamed: \"{old_name}\" → \"{new_name}\" ({count} records)')\n",
    "\n",
    "        self.stats['renamed_records'] = renamed_count\n",
    "\n",
    "        # Handle package items that need to be split\n",
    "        expanded_rows = []\n",
    "        rows_to_remove = []\n",
    "\n",
    "        for idx, row in standardized_df.iterrows():\n",
    "            item_lower = str(row['Item']).lower()\n",
    "\n",
    "            if item_lower in self.package_items:\n",
    "                # Mark this row for removal\n",
    "                rows_to_remove.append(idx)\n",
    "\n",
    "                # Create new rows for each component\n",
    "                components = self.package_items[item_lower]\n",
    "                original_qty = row['Quantity']\n",
    "\n",
    "                for component_item, qty_multiplier in components:\n",
    "                    new_row = row.copy()\n",
    "                    new_row['Item'] = component_item\n",
    "                    new_row['Quantity'] = original_qty * qty_multiplier\n",
    "                    expanded_rows.append(new_row)\n",
    "\n",
    "                print(f'  ✓ Expanded: \"{row[\"Item\"]}\" → {len(components)} items ({original_qty} qty each)')\n",
    "\n",
    "        # Remove package items\n",
    "        if rows_to_remove:\n",
    "            standardized_df = standardized_df.drop(rows_to_remove)\n",
    "            self.stats['expanded_packages'] = len(rows_to_remove)\n",
    "\n",
    "        # Add expanded rows\n",
    "        if expanded_rows:\n",
    "            expanded_df = pd.DataFrame(expanded_rows)\n",
    "            standardized_df = pd.concat([standardized_df, expanded_df], ignore_index=True)\n",
    "            self.stats['expanded_items'] = len(expanded_rows)\n",
    "\n",
    "        # Sort by date\n",
    "        standardized_df = standardized_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "        print(f'\\nStandardization Summary:')\n",
    "        print(f'  Simple renames: {renamed_count} records')\n",
    "        print(f'  Package expansions: {len(rows_to_remove)} packages → {len(expanded_rows)} items')\n",
    "        print(f'  Total records: {len(df)} → {len(standardized_df)}')\n",
    "\n",
    "        return standardized_df\n",
    "\n",
    "    def identify_discontinued_items(self, df: pd.DataFrame) -> Tuple[Set[str], pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Identify items in sales data that are not in menu BOM\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to check\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (set of discontinued item names, DataFrame with discontinued items stats, DataFrame with valid items)\n",
    "        \"\"\"\n",
    "        print('\\n' + '-' * 80)\n",
    "        print('STEP 2: IDENTIFYING DISCONTINUED ITEMS')\n",
    "        print('-' * 80)\n",
    "\n",
    "        # Get all unique items from sales data\n",
    "        sales_items = df['Item'].dropna().unique()\n",
    "\n",
    "        # Find discontinued items (in sales but not in menu BOM)\n",
    "        discontinued_items = set()\n",
    "        valid_items = set()\n",
    "\n",
    "        for item in sales_items:\n",
    "            normalized = self._normalize_item_name(item)\n",
    "            if normalized and normalized not in self.active_items:\n",
    "                discontinued_items.add(item)  # Store original name for display\n",
    "            else:\n",
    "                valid_items.add(item)\n",
    "\n",
    "        self.stats['discontinued_items'] = len(discontinued_items)\n",
    "\n",
    "        # Create statistics DataFrame for discontinued items\n",
    "        discontinued_stats = []\n",
    "        for item in discontinued_items:\n",
    "            item_data = df[df['Item'] == item]\n",
    "            stats = {\n",
    "                'Item': item,\n",
    "                'Total_Quantity_Sold': item_data['Quantity'].sum(),\n",
    "                'Total_Transactions': len(item_data),\n",
    "                'First_Sale_Date': item_data['Date'].min(),\n",
    "                'Last_Sale_Date': item_data['Date'].max()\n",
    "            }\n",
    "            discontinued_stats.append(stats)\n",
    "\n",
    "        discontinued_df = pd.DataFrame(discontinued_stats)\n",
    "\n",
    "        # Sort by total quantity sold (descending)\n",
    "        if not discontinued_df.empty:\n",
    "            discontinued_df = discontinued_df.sort_values('Total_Quantity_Sold', ascending=False)\n",
    "\n",
    "        print(f'\\n✓ Valid items (found in BOM): {len(valid_items)}')\n",
    "        print(f'✗ Discontinued items (NOT in BOM): {len(discontinued_items)}')\n",
    "\n",
    "        return discontinued_items, discontinued_df, df\n",
    "\n",
    "    def remove_discontinued_items(self, df: pd.DataFrame, discontinued_items: Set[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove discontinued items from sales data\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to clean\n",
    "            discontinued_items: Set of discontinued item names to remove\n",
    "\n",
    "        Returns:\n",
    "            Cleaned DataFrame with discontinued items removed\n",
    "        \"\"\"\n",
    "        # Filter out discontinued items\n",
    "        cleaned_df = df[~df['Item'].isin(discontinued_items)].copy()\n",
    "\n",
    "        self.stats['removed_records'] = len(df) - len(cleaned_df)\n",
    "\n",
    "        return cleaned_df\n",
    "\n",
    "    def save_cleaned_data(self, cleaned_df: pd.DataFrame, output_path: str):\n",
    "        \"\"\"\n",
    "        Save cleaned sales data to CSV\n",
    "\n",
    "        Args:\n",
    "            cleaned_df: Cleaned DataFrame\n",
    "            output_path: Path where cleaned CSV should be saved\n",
    "        \"\"\"\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        cleaned_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f'\\n✓ Cleaned data saved to: {output_path}')\n",
    "        print(f'  Total records: {len(cleaned_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SALES DATA CLEANER\n",
      "================================================================================\n",
      "\n",
      "Sales data: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data.csv\n",
      "Menu BOM: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/raw/bom/menu_bom.csv\n",
      "Output: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data_cleaned.csv\n",
      "\n",
      "Original records: 58,777\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 1: STANDARDIZING ITEM NAMES\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Renamed: \"lemon tea hot\" → \"Lemon Hot\" (1004 records)\n",
      "  ✓ Renamed: \"lemon tea ice\" → \"Lemon Ice\" (984 records)\n",
      "  ✓ Renamed: \"hot tea ajeng\" → \"Ajeng Hot\" (499 records)\n",
      "  ✓ Renamed: \"ice tea ajeng\" → \"Ajeng Ice\" (226 records)\n",
      "  ✓ Renamed: \"hot tea anindya\" → \"Anindya Hot\" (207 records)\n",
      "  ✓ Renamed: \"ice tea anindya\" → \"Anindya Ice\" (112 records)\n",
      "  ✓ Renamed: \"kopi susu panas\" → \"Kopi Susu Hot\" (715 records)\n",
      "  ✓ Renamed: \"ice cappucino\" → \"Cappucino Ice\" (264 records)\n",
      "  ✓ Renamed: \"latte\" → \"Latte Hot\" (242 records)\n",
      "  ✓ Renamed: \"ice latte\" → \"Latte Ice\" (120 records)\n",
      "  ✓ Renamed: \"picolo\" → \"Piccolo\" (206 records)\n",
      "  ✓ Renamed: \"long black hot\" → \"Black Hot\" (54 records)\n",
      "  ✓ Renamed: \"long black ice\" → \"Black Ice\" (50 records)\n",
      "  ✓ Renamed: \"red velvet\" → \"Red Velvet Ice\" (139 records)\n",
      "  ✓ Renamed: \"susu coklat\" → \"Coklat Hot\" (1 records)\n",
      "  ✓ Renamed: \"v60 hacienda natural\" → \"v60\" (220 records)\n",
      "  ✓ Renamed: \"v60 argopuro\" → \"v60\" (60 records)\n",
      "  ✓ Renamed: \"v60 finca\" → \"v60\" (45 records)\n",
      "  ✓ Renamed: \"mie goreng telur\" → \"Mie Goreng\" (764 records)\n",
      "  ✓ Renamed: \"mie rebus telur\" → \"Mie Rebus\" (324 records)\n",
      "  ✓ Renamed: \"nasi goreng djawa\" → \"Nasi Goreng Jawa\" (686 records)\n",
      "  ✓ Renamed: \"nasi ayam daun jeruk\" → \"Ayam Daun Jeruk\" (197 records)\n",
      "  ✓ Renamed: \"nasi ayam rempah\" → \"Ayam Rempah\" (44 records)\n",
      "  ✓ Renamed: \"ayam mentega (paket)\" → \"Ayam Mentega\" (34 records)\n",
      "  ✓ Renamed: \"ayam curry (paket)\" → \"Ayam Curry\" (36 records)\n",
      "  ✓ Renamed: \"cireng isi\" → \"Cireng\" (44 records)\n",
      "  ✓ Renamed: \"pannacotta\" → \"Panacotta\" (311 records)\n",
      "  ✓ Renamed: \"add vanilla ice cream\" → \"Add Ice Cream Vanilla\" (178 records)\n",
      "  ✓ Renamed: \"add telur ceplok\" → \"Add Telur\" (175 records)\n",
      "  ✓ Renamed: \"add telur dadar\" → \"Add Telur\" (41 records)\n",
      "  ✓ Renamed: \"add caramel topping\" → \"Add Topping Caramel\" (29 records)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (4.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (13.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (3.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (3.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (6.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (6.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (7.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (20.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (3.0 qty each)\n",
      "\n",
      "Standardization Summary:\n",
      "  Simple renames: 8011 records\n",
      "  Package expansions: 36 packages → 72 items\n",
      "  Total records: 58777 → 58813\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 2: IDENTIFYING DISCONTINUED ITEMS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Valid items (found in BOM): 110\n",
      "✗ Discontinued items (NOT in BOM): 138\n",
      "\n",
      "Total discontinued items: 138\n",
      "Total transactions affected: 4604\n",
      "Total quantity sold: 5361\n",
      "\n",
      "Top 10 discontinued items:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item</th>\n",
       "      <th>Total_Quantity_Sold</th>\n",
       "      <th>Total_Transactions</th>\n",
       "      <th>First_Sale_Date</th>\n",
       "      <th>Last_Sale_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Americano Ice</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>981</td>\n",
       "      <td>2022-01-05 17:05:00</td>\n",
       "      <td>2025-05-13 18:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Air mineral</td>\n",
       "      <td>655.0</td>\n",
       "      <td>440</td>\n",
       "      <td>2022-01-25 14:54:00</td>\n",
       "      <td>2025-05-12 22:27:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Americano Hot</td>\n",
       "      <td>399.0</td>\n",
       "      <td>376</td>\n",
       "      <td>2022-01-10 18:49:00</td>\n",
       "      <td>2025-05-13 23:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Ayam suwir</td>\n",
       "      <td>365.0</td>\n",
       "      <td>342</td>\n",
       "      <td>2022-10-19 16:41:00</td>\n",
       "      <td>2025-05-12 20:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Rose Tea Hot</td>\n",
       "      <td>283.0</td>\n",
       "      <td>277</td>\n",
       "      <td>2022-08-17 11:54:00</td>\n",
       "      <td>2025-04-17 23:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Air Mineral</td>\n",
       "      <td>261.0</td>\n",
       "      <td>235</td>\n",
       "      <td>2023-01-06 18:20:00</td>\n",
       "      <td>2025-09-24 14:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Nasi</td>\n",
       "      <td>175.0</td>\n",
       "      <td>148</td>\n",
       "      <td>2022-01-03 20:48:00</td>\n",
       "      <td>2025-05-12 17:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Air Mineral besar</td>\n",
       "      <td>165.0</td>\n",
       "      <td>142</td>\n",
       "      <td>2024-12-14 17:29:00</td>\n",
       "      <td>2025-05-12 19:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>PANNACOTTA 50 cup</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-06-28 16:15:00</td>\n",
       "      <td>2023-07-14 14:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Rose Tea Ice</td>\n",
       "      <td>132.0</td>\n",
       "      <td>129</td>\n",
       "      <td>2024-05-01 14:08:00</td>\n",
       "      <td>2025-04-15 21:49:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Item  Total_Quantity_Sold  Total_Transactions  \\\n",
       "38       Americano Ice               1003.0                 981   \n",
       "73         Air mineral                655.0                 440   \n",
       "137      Americano Hot                399.0                 376   \n",
       "129         Ayam suwir                365.0                 342   \n",
       "114       Rose Tea Hot                283.0                 277   \n",
       "63         Air Mineral                261.0                 235   \n",
       "86                Nasi                175.0                 148   \n",
       "40   Air Mineral besar                165.0                 142   \n",
       "102  PANNACOTTA 50 cup                160.0                   3   \n",
       "132       Rose Tea Ice                132.0                 129   \n",
       "\n",
       "         First_Sale_Date       Last_Sale_Date  \n",
       "38   2022-01-05 17:05:00  2025-05-13 18:08:00  \n",
       "73   2022-01-25 14:54:00  2025-05-12 22:27:00  \n",
       "137  2022-01-10 18:49:00  2025-05-13 23:43:00  \n",
       "129  2022-10-19 16:41:00  2025-05-12 20:01:00  \n",
       "114  2022-08-17 11:54:00  2025-04-17 23:12:00  \n",
       "63   2023-01-06 18:20:00  2025-09-24 14:43:00  \n",
       "86   2022-01-03 20:48:00  2025-05-12 17:45:00  \n",
       "40   2024-12-14 17:29:00  2025-05-12 19:44:00  \n",
       "102  2023-06-28 16:15:00  2023-07-14 14:38:00  \n",
       "132  2024-05-01 14:08:00  2025-04-15 21:49:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 3: HANDLING DISCONTINUED ITEMS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Removing discontinued items...\n",
      "✓ Removed 4,604 records\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 4: SAVING CLEANED DATA\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Cleaned data saved to: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data_cleaned.csv\n",
      "  Total records: 54209\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Original records: 58,777\n",
      "\n",
      "Standardization:\n",
      "  - Renamed records: 8,011\n",
      "  - Expanded packages: 36 → 72 items\n",
      "\n",
      "Discontinued items:\n",
      "  - Items found: 138\n",
      "  - Records removed: 4,604\n",
      "\n",
      "Final records: 54,245\n",
      "================================================================================\n",
      "\n",
      "✓ Sales data cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Execute the sales data cleaner\n",
    "base_path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "sales_path = os.path.join(base_path, 'data', 'processed', 'sales_data.csv')\n",
    "menu_bom_path = os.path.join(base_path, 'data', 'raw', 'bom', 'menu_bom.csv')\n",
    "output_path = os.path.join(base_path, 'data', 'processed', 'sales_data_cleaned.csv')\n",
    "\n",
    "print('=' * 80)\n",
    "print('SALES DATA CLEANER')\n",
    "print('=' * 80)\n",
    "print(f'\\nSales data: {sales_path}')\n",
    "print(f'Menu BOM: {menu_bom_path}')\n",
    "print(f'Output: {output_path}')\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = SalesDataCleaner(sales_path, menu_bom_path)\n",
    "\n",
    "print(f'\\nOriginal records: {len(cleaner.sales_df):,}')\n",
    "\n",
    "# Step 1: Standardize names\n",
    "standardized_df = cleaner.standardize_names(cleaner.sales_df)\n",
    "\n",
    "# Step 2: Identify discontinued items\n",
    "discontinued_items, discontinued_df, current_df = cleaner.identify_discontinued_items(standardized_df)\n",
    "\n",
    "# Print discontinued items report\n",
    "if discontinued_items:\n",
    "    print(f'\\nTotal discontinued items: {len(discontinued_items)}')\n",
    "    print(f'Total transactions affected: {discontinued_df[\"Total_Transactions\"].sum():.0f}')\n",
    "    print(f'Total quantity sold: {discontinued_df[\"Total_Quantity_Sold\"].sum():.0f}')\n",
    "    \n",
    "    # Display top discontinued items\n",
    "    print('\\nTop 10 discontinued items:')\n",
    "    display(discontinued_df.head(10))\n",
    "else:\n",
    "    print('\\n✓ No discontinued items found!')\n",
    "\n",
    "# Step 3: Handle discontinued items (removing them)\n",
    "print('\\n' + '-' * 80)\n",
    "print('STEP 3: HANDLING DISCONTINUED ITEMS')\n",
    "print('-' * 80)\n",
    "\n",
    "if discontinued_items:\n",
    "    print('\\nRemoving discontinued items...')\n",
    "    final_df = cleaner.remove_discontinued_items(current_df, discontinued_items)\n",
    "    print(f'✓ Removed {cleaner.stats[\"removed_records\"]:,} records')\n",
    "else:\n",
    "    print('\\n✓ No discontinued items to remove')\n",
    "    final_df = current_df\n",
    "\n",
    "# Save cleaned data\n",
    "print('\\n' + '-' * 80)\n",
    "print('STEP 4: SAVING CLEANED DATA')\n",
    "print('-' * 80)\n",
    "cleaner.save_cleaned_data(final_df, output_path)\n",
    "\n",
    "# Print final summary\n",
    "print('\\n' + '=' * 80)\n",
    "print('FINAL SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nOriginal records: {cleaner.stats[\"original_records\"]:,}')\n",
    "print(f'\\nStandardization:')\n",
    "print(f'  - Renamed records: {cleaner.stats[\"renamed_records\"]:,}')\n",
    "print(f'  - Expanded packages: {cleaner.stats[\"expanded_packages\"]:,} → {cleaner.stats[\"expanded_items\"]:,} items')\n",
    "\n",
    "print(f'\\nDiscontinued items:')\n",
    "print(f'  - Items found: {cleaner.stats[\"discontinued_items\"]:,}')\n",
    "print(f'  - Records removed: {cleaner.stats[\"removed_records\"]:,}')\n",
    "\n",
    "final_records = cleaner.stats[\"original_records\"] + cleaner.stats[\"expanded_items\"] - cleaner.stats[\"removed_records\"]\n",
    "print(f'\\nFinal records: {final_records:,}')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\n✓ Sales data cleaning complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Raw Materials Preprocessor\n",
    "\n",
    "This section processes sales data and BOM files to calculate daily raw material requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawMaterialProcessor:\n",
    "    \"\"\"Process sales data and BOM files to calculate raw material requirements\"\"\"\n",
    "\n",
    "    def __init__(self, sales_path: str, menu_bom_path: str, condiment_bom_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the processor with file paths\n",
    "\n",
    "        Args:\n",
    "            sales_path: Path to sales data CSV\n",
    "            menu_bom_path: Path to menu BOM CSV\n",
    "            condiment_bom_path: Path to condiment BOM CSV\n",
    "        \"\"\"\n",
    "        self.sales_df = pd.read_csv(sales_path)\n",
    "        self.menu_bom_df = pd.read_csv(menu_bom_path)\n",
    "        self.condiment_bom_df = pd.read_csv(condiment_bom_path)\n",
    "\n",
    "        # Normalize item names in BOMs for matching\n",
    "        self.menu_bom_df['Item_normalized'] = self.menu_bom_df['Item'].str.strip().str.lower()\n",
    "\n",
    "        # Build item name mapping for fuzzy matching\n",
    "        self.item_name_map = self._build_item_name_map()\n",
    "\n",
    "        # Build condiment lookup dictionary for fast access\n",
    "        self.condiment_dict = self._build_condiment_dict()\n",
    "\n",
    "        # Cache for memoization of condiment expansion\n",
    "        self.expansion_cache: Dict[Tuple[str, float], Dict[str, float]] = {}\n",
    "\n",
    "    def _build_item_name_map(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Build a dictionary mapping normalized item names to original names in BOM\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with normalized names as keys and original BOM names as values\n",
    "        \"\"\"\n",
    "        name_map = {}\n",
    "        for item_name in self.menu_bom_df['Item'].unique():\n",
    "            normalized = item_name.strip().lower()\n",
    "            name_map[normalized] = item_name\n",
    "        return name_map\n",
    "\n",
    "    def _build_condiment_dict(self) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Build a dictionary mapping condiment names to their ingredients\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with condiment names as keys and DataFrames of ingredients as values\n",
    "        \"\"\"\n",
    "        condiment_dict = {}\n",
    "        for condiment_name in self.condiment_bom_df['Condiment'].unique():\n",
    "            condiment_dict[condiment_name] = self.condiment_bom_df[\n",
    "                self.condiment_bom_df['Condiment'] == condiment_name\n",
    "            ].copy()\n",
    "        return condiment_dict\n",
    "\n",
    "    def _is_condiment(self, ingredient_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if an ingredient is a condiment (needs further expansion)\n",
    "\n",
    "        Args:\n",
    "            ingredient_name: Name of the ingredient to check\n",
    "\n",
    "        Returns:\n",
    "            True if ingredient is a condiment, False otherwise\n",
    "        \"\"\"\n",
    "        return ingredient_name in self.condiment_dict\n",
    "\n",
    "    def _expand_condiment(self, condiment_name: str, quantity: float,\n",
    "                         condiment_unit: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Recursively expand a condiment into raw materials\n",
    "\n",
    "        Args:\n",
    "            condiment_name: Name of the condiment to expand\n",
    "            quantity: Quantity of condiment needed\n",
    "            condiment_unit: Unit of the condiment\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping raw material names to quantities\n",
    "        \"\"\"\n",
    "        # Check cache first\n",
    "        cache_key = (condiment_name, quantity)\n",
    "        if cache_key in self.expansion_cache:\n",
    "            return self.expansion_cache[cache_key].copy()\n",
    "\n",
    "        raw_materials = {}\n",
    "\n",
    "        if condiment_name not in self.condiment_dict:\n",
    "            # Not a condiment, treat as raw material\n",
    "            raw_materials[condiment_name] = quantity\n",
    "            return raw_materials\n",
    "\n",
    "        # Get condiment recipe\n",
    "        condiment_recipe = self.condiment_dict[condiment_name]\n",
    "\n",
    "        # Get the base quantity that the recipe makes\n",
    "        base_condiment_qty = condiment_recipe['Condiment_Qty'].iloc[0]\n",
    "\n",
    "        # Calculate scaling factor\n",
    "        scaling_factor = quantity / base_condiment_qty\n",
    "\n",
    "        # Expand each sub-ingredient\n",
    "        for _, row in condiment_recipe.iterrows():\n",
    "            sub_ingredient = row['Sub_Ingredient']\n",
    "            sub_qty = row['Qty_per_condiment_unit']\n",
    "            sub_unit = row['Sub_Unit']\n",
    "\n",
    "            # Convert quantity to float\n",
    "            try:\n",
    "                sub_qty = float(sub_qty)\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Invalid quantity '{sub_qty}' for sub-ingredient '{sub_ingredient}' in condiment '{condiment_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate actual quantity needed\n",
    "            actual_qty = sub_qty * scaling_factor\n",
    "\n",
    "            # Recursively expand if it's another condiment\n",
    "            if self._is_condiment(sub_ingredient):\n",
    "                sub_materials = self._expand_condiment(sub_ingredient, actual_qty, sub_unit)\n",
    "                # Aggregate the results\n",
    "                for material, material_qty in sub_materials.items():\n",
    "                    raw_materials[material] = raw_materials.get(material, 0) + material_qty\n",
    "            else:\n",
    "                # It's a raw material\n",
    "                raw_materials[sub_ingredient] = raw_materials.get(sub_ingredient, 0) + actual_qty\n",
    "\n",
    "        # Cache the result\n",
    "        self.expansion_cache[cache_key] = raw_materials.copy()\n",
    "\n",
    "        return raw_materials\n",
    "\n",
    "    def _get_item_raw_materials(self, item_name: str, quantity: float) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Get all raw materials needed for a menu item\n",
    "\n",
    "        Args:\n",
    "            item_name: Name of the menu item\n",
    "            quantity: Quantity of items sold\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping raw material names to quantities\n",
    "        \"\"\"\n",
    "        raw_materials = {}\n",
    "\n",
    "        # Try to normalize item name for matching\n",
    "        normalized_name = item_name.strip().lower()\n",
    "\n",
    "        # Check if we have a mapping for this normalized name\n",
    "        if normalized_name in self.item_name_map:\n",
    "            matched_name = self.item_name_map[normalized_name]\n",
    "            item_recipe = self.menu_bom_df[self.menu_bom_df['Item'] == matched_name]\n",
    "        else:\n",
    "            # Try exact match\n",
    "            item_recipe = self.menu_bom_df[self.menu_bom_df['Item'] == item_name]\n",
    "\n",
    "        if item_recipe.empty:\n",
    "            # Item not found in BOM, skip with warning\n",
    "            print(f\"Warning: Item '{item_name}' not found in menu BOM\")\n",
    "            return raw_materials\n",
    "\n",
    "        # Process each ingredient in the recipe\n",
    "        for _, row in item_recipe.iterrows():\n",
    "            ingredient = row['Bahan']\n",
    "            ingredient_qty = row['Qty']\n",
    "            ingredient_unit = row['Unit']\n",
    "\n",
    "            # Convert quantity to float to handle string values\n",
    "            try:\n",
    "                ingredient_qty = float(ingredient_qty)\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Invalid quantity '{ingredient_qty}' for ingredient '{ingredient}' in item '{item_name}'. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Calculate total quantity needed\n",
    "            total_qty = ingredient_qty * quantity\n",
    "\n",
    "            # Check if ingredient is a condiment and needs expansion\n",
    "            if self._is_condiment(ingredient):\n",
    "                sub_materials = self._expand_condiment(ingredient, total_qty, ingredient_unit)\n",
    "                # Aggregate the results\n",
    "                for material, material_qty in sub_materials.items():\n",
    "                    raw_materials[material] = raw_materials.get(material, 0) + material_qty\n",
    "            else:\n",
    "                # It's a raw material\n",
    "                raw_materials[ingredient] = raw_materials.get(ingredient, 0) + total_qty\n",
    "\n",
    "        return raw_materials\n",
    "\n",
    "    def _normalize_material_name(self, material_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize material names to handle case inconsistencies\n",
    "\n",
    "        Args:\n",
    "            material_name: Raw material name to normalize\n",
    "\n",
    "        Returns:\n",
    "            Normalized material name with consistent casing\n",
    "        \"\"\"\n",
    "        # Strip whitespace\n",
    "        normalized = material_name.strip()\n",
    "\n",
    "        # Handle common acronyms that should be uppercase\n",
    "        acronyms = ['SKM', 'BSJ', 'SP']\n",
    "        upper_name = normalized.upper()\n",
    "        if upper_name in acronyms:\n",
    "            return upper_name\n",
    "\n",
    "        # For other materials, use title case for consistency\n",
    "        return normalized.title()\n",
    "\n",
    "    def process_sales_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process sales data to calculate daily raw material requirements\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: Date, Raw_Material, Quantity_Required\n",
    "        \"\"\"\n",
    "        # Convert Date column to datetime and extract just the date\n",
    "        self.sales_df['Date'] = pd.to_datetime(self.sales_df['Date']).dt.date\n",
    "\n",
    "        # Dictionary to store daily raw material requirements\n",
    "        # Structure: {(date, raw_material): quantity}\n",
    "        daily_requirements = {}\n",
    "\n",
    "        # Process each sales transaction\n",
    "        total_rows = len(self.sales_df)\n",
    "        for idx, row in self.sales_df.iterrows():\n",
    "            if idx % 100 == 0:\n",
    "                print(f\"Processing row {idx + 1}/{total_rows}...\")\n",
    "\n",
    "            date = row['Date']\n",
    "            item = row['Item']\n",
    "            quantity = row['Quantity']\n",
    "\n",
    "            # Skip if quantity is invalid\n",
    "            if pd.isna(quantity) or quantity <= 0:\n",
    "                continue\n",
    "\n",
    "            # Get raw materials for this item\n",
    "            raw_materials = self._get_item_raw_materials(item, quantity)\n",
    "\n",
    "            # Aggregate by date and raw material (with normalized names)\n",
    "            for material, material_qty in raw_materials.items():\n",
    "                normalized_material = self._normalize_material_name(material)\n",
    "                key = (date, normalized_material)\n",
    "                daily_requirements[key] = daily_requirements.get(key, 0) + material_qty\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        result_data = []\n",
    "        for (date, material), qty in daily_requirements.items():\n",
    "            result_data.append({\n",
    "                'Date': date,\n",
    "                'Raw_Material': material,\n",
    "                'Quantity_Required': qty\n",
    "            })\n",
    "\n",
    "        result_df = pd.DataFrame(result_data)\n",
    "\n",
    "        # Sort by date and raw material\n",
    "        result_df = result_df.sort_values(['Date', 'Raw_Material']).reset_index(drop=True)\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def save_results(self, output_path: str):\n",
    "        \"\"\"\n",
    "        Process sales data and save results to CSV\n",
    "\n",
    "        Args:\n",
    "            output_path: Path where the output CSV should be saved\n",
    "        \"\"\"\n",
    "        print(\"Starting raw material requirements processing...\")\n",
    "        result_df = self.process_sales_data()\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f\"\\nProcessing complete!\")\n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        print(f\"Total unique dates: {result_df['Date'].nunique()}\")\n",
    "        print(f\"Total unique raw materials: {result_df['Raw_Material'].nunique()}\")\n",
    "        print(f\"Total rows in output: {len(result_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting raw material requirements processing...\n",
      "Processing row 1/54209...\n",
      "Processing row 101/54209...\n",
      "Processing row 201/54209...\n",
      "Processing row 301/54209...\n",
      "Processing row 401/54209...\n",
      "Processing row 501/54209...\n",
      "Processing row 601/54209...\n",
      "Processing row 701/54209...\n",
      "Processing row 801/54209...\n",
      "Processing row 901/54209...\n",
      "Processing row 1001/54209...\n",
      "Processing row 1101/54209...\n",
      "Processing row 1201/54209...\n",
      "Processing row 1301/54209...\n",
      "Processing row 1401/54209...\n",
      "Processing row 1501/54209...\n",
      "Processing row 1601/54209...\n",
      "Processing row 1701/54209...\n",
      "Processing row 1801/54209...\n",
      "Processing row 1901/54209...\n",
      "Processing row 2001/54209...\n",
      "Processing row 2101/54209...\n",
      "Processing row 2201/54209...\n",
      "Processing row 2301/54209...\n",
      "Processing row 2401/54209...\n",
      "Processing row 2501/54209...\n",
      "Processing row 2601/54209...\n",
      "Processing row 2701/54209...\n",
      "Processing row 2801/54209...\n",
      "Processing row 2901/54209...\n",
      "Processing row 3001/54209...\n",
      "Processing row 3101/54209...\n",
      "Processing row 3201/54209...\n",
      "Processing row 3301/54209...\n",
      "Processing row 3401/54209...\n",
      "Processing row 3501/54209...\n",
      "Processing row 3601/54209...\n",
      "Processing row 3701/54209...\n",
      "Processing row 3801/54209...\n",
      "Processing row 3901/54209...\n",
      "Processing row 4001/54209...\n",
      "Processing row 4101/54209...\n",
      "Processing row 4201/54209...\n",
      "Processing row 4301/54209...\n",
      "Processing row 4401/54209...\n",
      "Processing row 4501/54209...\n",
      "Processing row 4601/54209...\n",
      "Processing row 4701/54209...\n",
      "Processing row 4801/54209...\n",
      "Processing row 4901/54209...\n",
      "Processing row 5001/54209...\n",
      "Processing row 5101/54209...\n",
      "Processing row 5201/54209...\n",
      "Processing row 5301/54209...\n",
      "Processing row 5401/54209...\n",
      "Processing row 5501/54209...\n",
      "Processing row 5601/54209...\n",
      "Processing row 5701/54209...\n",
      "Processing row 5801/54209...\n",
      "Processing row 5901/54209...\n",
      "Processing row 6001/54209...\n",
      "Processing row 6101/54209...\n",
      "Processing row 6201/54209...\n",
      "Processing row 6301/54209...\n",
      "Processing row 6401/54209...\n",
      "Processing row 6501/54209...\n",
      "Processing row 6601/54209...\n",
      "Processing row 6701/54209...\n",
      "Processing row 6801/54209...\n",
      "Processing row 6901/54209...\n",
      "Processing row 7001/54209...\n",
      "Processing row 7101/54209...\n",
      "Processing row 7201/54209...\n",
      "Processing row 7301/54209...\n",
      "Processing row 7401/54209...\n",
      "Processing row 7501/54209...\n",
      "Processing row 7601/54209...\n",
      "Processing row 7701/54209...\n",
      "Processing row 7801/54209...\n",
      "Processing row 7901/54209...\n",
      "Processing row 8001/54209...\n",
      "Processing row 8101/54209...\n",
      "Processing row 8201/54209...\n",
      "Processing row 8301/54209...\n",
      "Processing row 8401/54209...\n",
      "Processing row 8501/54209...\n",
      "Processing row 8601/54209...\n",
      "Processing row 8701/54209...\n",
      "Processing row 8801/54209...\n",
      "Processing row 8901/54209...\n",
      "Processing row 9001/54209...\n",
      "Processing row 9101/54209...\n",
      "Processing row 9201/54209...\n",
      "Processing row 9301/54209...\n",
      "Processing row 9401/54209...\n",
      "Processing row 9501/54209...\n",
      "Processing row 9601/54209...\n",
      "Processing row 9701/54209...\n",
      "Processing row 9801/54209...\n",
      "Processing row 9901/54209...\n",
      "Processing row 10001/54209...\n",
      "Processing row 10101/54209...\n",
      "Processing row 10201/54209...\n",
      "Processing row 10301/54209...\n",
      "Processing row 10401/54209...\n",
      "Processing row 10501/54209...\n",
      "Processing row 10601/54209...\n",
      "Processing row 10701/54209...\n",
      "Processing row 10801/54209...\n",
      "Processing row 10901/54209...\n",
      "Processing row 11001/54209...\n",
      "Processing row 11101/54209...\n",
      "Processing row 11201/54209...\n",
      "Processing row 11301/54209...\n",
      "Processing row 11401/54209...\n",
      "Processing row 11501/54209...\n",
      "Processing row 11601/54209...\n",
      "Processing row 11701/54209...\n",
      "Processing row 11801/54209...\n",
      "Processing row 11901/54209...\n",
      "Processing row 12001/54209...\n",
      "Processing row 12101/54209...\n",
      "Processing row 12201/54209...\n",
      "Processing row 12301/54209...\n",
      "Processing row 12401/54209...\n",
      "Processing row 12501/54209...\n",
      "Processing row 12601/54209...\n",
      "Processing row 12701/54209...\n",
      "Processing row 12801/54209...\n",
      "Processing row 12901/54209...\n",
      "Processing row 13001/54209...\n",
      "Processing row 13101/54209...\n",
      "Processing row 13201/54209...\n",
      "Processing row 13301/54209...\n",
      "Processing row 13401/54209...\n",
      "Processing row 13501/54209...\n",
      "Processing row 13601/54209...\n",
      "Processing row 13701/54209...\n",
      "Processing row 13801/54209...\n",
      "Processing row 13901/54209...\n",
      "Processing row 14001/54209...\n",
      "Processing row 14101/54209...\n",
      "Processing row 14201/54209...\n",
      "Processing row 14301/54209...\n",
      "Processing row 14401/54209...\n",
      "Processing row 14501/54209...\n",
      "Processing row 14601/54209...\n",
      "Processing row 14701/54209...\n",
      "Processing row 14801/54209...\n",
      "Processing row 14901/54209...\n",
      "Processing row 15001/54209...\n",
      "Processing row 15101/54209...\n",
      "Processing row 15201/54209...\n",
      "Processing row 15301/54209...\n",
      "Processing row 15401/54209...\n",
      "Processing row 15501/54209...\n",
      "Processing row 15601/54209...\n",
      "Processing row 15701/54209...\n",
      "Processing row 15801/54209...\n",
      "Processing row 15901/54209...\n",
      "Processing row 16001/54209...\n",
      "Processing row 16101/54209...\n",
      "Processing row 16201/54209...\n",
      "Processing row 16301/54209...\n",
      "Processing row 16401/54209...\n",
      "Processing row 16501/54209...\n",
      "Processing row 16601/54209...\n",
      "Processing row 16701/54209...\n",
      "Processing row 16801/54209...\n",
      "Processing row 16901/54209...\n",
      "Processing row 17001/54209...\n",
      "Processing row 17101/54209...\n",
      "Processing row 17201/54209...\n",
      "Processing row 17301/54209...\n",
      "Processing row 17401/54209...\n",
      "Processing row 17501/54209...\n",
      "Processing row 17601/54209...\n",
      "Processing row 17701/54209...\n",
      "Processing row 17801/54209...\n",
      "Processing row 17901/54209...\n",
      "Processing row 18001/54209...\n",
      "Processing row 18101/54209...\n",
      "Processing row 18201/54209...\n",
      "Processing row 18301/54209...\n",
      "Processing row 18401/54209...\n",
      "Processing row 18501/54209...\n",
      "Processing row 18601/54209...\n",
      "Processing row 18701/54209...\n",
      "Processing row 18801/54209...\n",
      "Processing row 18901/54209...\n",
      "Processing row 19001/54209...\n",
      "Processing row 19101/54209...\n",
      "Processing row 19201/54209...\n",
      "Processing row 19301/54209...\n",
      "Processing row 19401/54209...\n",
      "Processing row 19501/54209...\n",
      "Processing row 19601/54209...\n",
      "Processing row 19701/54209...\n",
      "Processing row 19801/54209...\n",
      "Processing row 19901/54209...\n",
      "Processing row 20001/54209...\n",
      "Processing row 20101/54209...\n",
      "Processing row 20201/54209...\n",
      "Processing row 20301/54209...\n",
      "Processing row 20401/54209...\n",
      "Processing row 20501/54209...\n",
      "Processing row 20601/54209...\n",
      "Processing row 20701/54209...\n",
      "Processing row 20801/54209...\n",
      "Processing row 20901/54209...\n",
      "Processing row 21001/54209...\n",
      "Processing row 21101/54209...\n",
      "Processing row 21201/54209...\n",
      "Processing row 21301/54209...\n",
      "Processing row 21401/54209...\n",
      "Processing row 21501/54209...\n",
      "Processing row 21601/54209...\n",
      "Processing row 21701/54209...\n",
      "Processing row 21801/54209...\n",
      "Processing row 21901/54209...\n",
      "Processing row 22001/54209...\n",
      "Processing row 22101/54209...\n",
      "Processing row 22201/54209...\n",
      "Processing row 22301/54209...\n",
      "Processing row 22401/54209...\n",
      "Processing row 22501/54209...\n",
      "Processing row 22601/54209...\n",
      "Processing row 22701/54209...\n",
      "Processing row 22801/54209...\n",
      "Processing row 22901/54209...\n",
      "Processing row 23001/54209...\n",
      "Processing row 23101/54209...\n",
      "Processing row 23201/54209...\n",
      "Processing row 23301/54209...\n",
      "Processing row 23401/54209...\n",
      "Processing row 23501/54209...\n",
      "Processing row 23601/54209...\n",
      "Processing row 23701/54209...\n",
      "Processing row 23801/54209...\n",
      "Processing row 23901/54209...\n",
      "Processing row 24001/54209...\n",
      "Processing row 24101/54209...\n",
      "Processing row 24201/54209...\n",
      "Processing row 24301/54209...\n",
      "Processing row 24401/54209...\n",
      "Processing row 24501/54209...\n",
      "Processing row 24601/54209...\n",
      "Processing row 24701/54209...\n",
      "Processing row 24801/54209...\n",
      "Processing row 24901/54209...\n",
      "Processing row 25001/54209...\n",
      "Processing row 25101/54209...\n",
      "Processing row 25201/54209...\n",
      "Processing row 25301/54209...\n",
      "Processing row 25401/54209...\n",
      "Processing row 25501/54209...\n",
      "Processing row 25601/54209...\n",
      "Processing row 25701/54209...\n",
      "Processing row 25801/54209...\n",
      "Processing row 25901/54209...\n",
      "Processing row 26001/54209...\n",
      "Processing row 26101/54209...\n",
      "Processing row 26201/54209...\n",
      "Processing row 26301/54209...\n",
      "Processing row 26401/54209...\n",
      "Processing row 26501/54209...\n",
      "Processing row 26601/54209...\n",
      "Processing row 26701/54209...\n",
      "Processing row 26801/54209...\n",
      "Processing row 26901/54209...\n",
      "Processing row 27001/54209...\n",
      "Processing row 27101/54209...\n",
      "Processing row 27201/54209...\n",
      "Processing row 27301/54209...\n",
      "Processing row 27401/54209...\n",
      "Processing row 27501/54209...\n",
      "Processing row 27601/54209...\n",
      "Processing row 27701/54209...\n",
      "Processing row 27801/54209...\n",
      "Processing row 27901/54209...\n",
      "Processing row 28001/54209...\n",
      "Processing row 28101/54209...\n",
      "Processing row 28201/54209...\n",
      "Processing row 28301/54209...\n",
      "Processing row 28401/54209...\n",
      "Processing row 28501/54209...\n",
      "Processing row 28601/54209...\n",
      "Processing row 28701/54209...\n",
      "Processing row 28801/54209...\n",
      "Processing row 28901/54209...\n",
      "Processing row 29001/54209...\n",
      "Processing row 29101/54209...\n",
      "Processing row 29201/54209...\n",
      "Processing row 29301/54209...\n",
      "Processing row 29401/54209...\n",
      "Processing row 29501/54209...\n",
      "Processing row 29601/54209...\n",
      "Processing row 29701/54209...\n",
      "Processing row 29801/54209...\n",
      "Processing row 29901/54209...\n",
      "Processing row 30001/54209...\n",
      "Processing row 30101/54209...\n",
      "Processing row 30201/54209...\n",
      "Processing row 30301/54209...\n",
      "Processing row 30401/54209...\n",
      "Processing row 30501/54209...\n",
      "Processing row 30601/54209...\n",
      "Processing row 30701/54209...\n",
      "Processing row 30801/54209...\n",
      "Processing row 30901/54209...\n",
      "Processing row 31001/54209...\n",
      "Processing row 31101/54209...\n",
      "Processing row 31201/54209...\n",
      "Processing row 31301/54209...\n",
      "Processing row 31401/54209...\n",
      "Processing row 31501/54209...\n",
      "Processing row 31601/54209...\n",
      "Processing row 31701/54209...\n",
      "Processing row 31801/54209...\n",
      "Processing row 31901/54209...\n",
      "Processing row 32001/54209...\n",
      "Processing row 32101/54209...\n",
      "Processing row 32201/54209...\n",
      "Processing row 32301/54209...\n",
      "Processing row 32401/54209...\n",
      "Processing row 32501/54209...\n",
      "Processing row 32601/54209...\n",
      "Processing row 32701/54209...\n",
      "Processing row 32801/54209...\n",
      "Processing row 32901/54209...\n",
      "Processing row 33001/54209...\n",
      "Processing row 33101/54209...\n",
      "Processing row 33201/54209...\n",
      "Processing row 33301/54209...\n",
      "Processing row 33401/54209...\n",
      "Processing row 33501/54209...\n",
      "Processing row 33601/54209...\n",
      "Processing row 33701/54209...\n",
      "Processing row 33801/54209...\n",
      "Processing row 33901/54209...\n",
      "Processing row 34001/54209...\n",
      "Processing row 34101/54209...\n",
      "Processing row 34201/54209...\n",
      "Processing row 34301/54209...\n",
      "Processing row 34401/54209...\n",
      "Processing row 34501/54209...\n",
      "Processing row 34601/54209...\n",
      "Processing row 34701/54209...\n",
      "Processing row 34801/54209...\n",
      "Processing row 34901/54209...\n",
      "Processing row 35001/54209...\n",
      "Processing row 35101/54209...\n",
      "Processing row 35201/54209...\n",
      "Processing row 35301/54209...\n",
      "Processing row 35401/54209...\n",
      "Processing row 35501/54209...\n",
      "Processing row 35601/54209...\n",
      "Processing row 35701/54209...\n",
      "Processing row 35801/54209...\n",
      "Processing row 35901/54209...\n",
      "Processing row 36001/54209...\n",
      "Processing row 36101/54209...\n",
      "Processing row 36201/54209...\n",
      "Processing row 36301/54209...\n",
      "Processing row 36401/54209...\n",
      "Processing row 36501/54209...\n",
      "Processing row 36601/54209...\n",
      "Processing row 36701/54209...\n",
      "Processing row 36801/54209...\n",
      "Processing row 36901/54209...\n",
      "Processing row 37001/54209...\n",
      "Processing row 37101/54209...\n",
      "Processing row 37201/54209...\n",
      "Processing row 37301/54209...\n",
      "Processing row 37401/54209...\n",
      "Processing row 37501/54209...\n",
      "Processing row 37601/54209...\n",
      "Processing row 37701/54209...\n",
      "Processing row 37801/54209...\n",
      "Processing row 37901/54209...\n",
      "Processing row 38001/54209...\n",
      "Processing row 38101/54209...\n",
      "Processing row 38201/54209...\n",
      "Processing row 38301/54209...\n",
      "Processing row 38401/54209...\n",
      "Processing row 38501/54209...\n",
      "Processing row 38601/54209...\n",
      "Processing row 38701/54209...\n",
      "Processing row 38801/54209...\n",
      "Processing row 38901/54209...\n",
      "Processing row 39001/54209...\n",
      "Processing row 39101/54209...\n",
      "Processing row 39201/54209...\n",
      "Processing row 39301/54209...\n",
      "Processing row 39401/54209...\n",
      "Processing row 39501/54209...\n",
      "Processing row 39601/54209...\n",
      "Processing row 39701/54209...\n",
      "Processing row 39801/54209...\n",
      "Processing row 39901/54209...\n",
      "Processing row 40001/54209...\n",
      "Processing row 40101/54209...\n",
      "Processing row 40201/54209...\n",
      "Processing row 40301/54209...\n",
      "Processing row 40401/54209...\n",
      "Processing row 40501/54209...\n",
      "Processing row 40601/54209...\n",
      "Processing row 40701/54209...\n",
      "Processing row 40801/54209...\n",
      "Processing row 40901/54209...\n",
      "Processing row 41001/54209...\n",
      "Processing row 41101/54209...\n",
      "Processing row 41201/54209...\n",
      "Processing row 41301/54209...\n",
      "Processing row 41401/54209...\n",
      "Processing row 41501/54209...\n",
      "Processing row 41601/54209...\n",
      "Processing row 41701/54209...\n",
      "Processing row 41801/54209...\n",
      "Processing row 41901/54209...\n",
      "Processing row 42001/54209...\n",
      "Processing row 42101/54209...\n",
      "Processing row 42201/54209...\n",
      "Processing row 42301/54209...\n",
      "Processing row 42401/54209...\n",
      "Processing row 42501/54209...\n",
      "Processing row 42601/54209...\n",
      "Processing row 42701/54209...\n",
      "Processing row 42801/54209...\n",
      "Processing row 42901/54209...\n",
      "Processing row 43001/54209...\n",
      "Processing row 43101/54209...\n",
      "Processing row 43201/54209...\n",
      "Processing row 43301/54209...\n",
      "Processing row 43401/54209...\n",
      "Processing row 43501/54209...\n",
      "Processing row 43601/54209...\n",
      "Processing row 43701/54209...\n",
      "Processing row 43801/54209...\n",
      "Processing row 43901/54209...\n",
      "Processing row 44001/54209...\n",
      "Processing row 44101/54209...\n",
      "Processing row 44201/54209...\n",
      "Processing row 44301/54209...\n",
      "Processing row 44401/54209...\n",
      "Processing row 44501/54209...\n",
      "Processing row 44601/54209...\n",
      "Processing row 44701/54209...\n",
      "Processing row 44801/54209...\n",
      "Processing row 44901/54209...\n",
      "Processing row 45001/54209...\n",
      "Processing row 45101/54209...\n",
      "Processing row 45201/54209...\n",
      "Processing row 45301/54209...\n",
      "Processing row 45401/54209...\n",
      "Processing row 45501/54209...\n",
      "Processing row 45601/54209...\n",
      "Processing row 45701/54209...\n",
      "Processing row 45801/54209...\n",
      "Processing row 45901/54209...\n",
      "Processing row 46001/54209...\n",
      "Processing row 46101/54209...\n",
      "Processing row 46201/54209...\n",
      "Processing row 46301/54209...\n",
      "Processing row 46401/54209...\n",
      "Processing row 46501/54209...\n",
      "Processing row 46601/54209...\n",
      "Processing row 46701/54209...\n",
      "Processing row 46801/54209...\n",
      "Processing row 46901/54209...\n",
      "Processing row 47001/54209...\n",
      "Processing row 47101/54209...\n",
      "Processing row 47201/54209...\n",
      "Processing row 47301/54209...\n",
      "Processing row 47401/54209...\n",
      "Processing row 47501/54209...\n",
      "Processing row 47601/54209...\n",
      "Processing row 47701/54209...\n",
      "Processing row 47801/54209...\n",
      "Processing row 47901/54209...\n",
      "Processing row 48001/54209...\n",
      "Processing row 48101/54209...\n",
      "Processing row 48201/54209...\n",
      "Processing row 48301/54209...\n",
      "Processing row 48401/54209...\n",
      "Processing row 48501/54209...\n",
      "Processing row 48601/54209...\n",
      "Processing row 48701/54209...\n",
      "Processing row 48801/54209...\n",
      "Processing row 48901/54209...\n",
      "Processing row 49001/54209...\n",
      "Processing row 49101/54209...\n",
      "Processing row 49201/54209...\n",
      "Processing row 49301/54209...\n",
      "Processing row 49401/54209...\n",
      "Processing row 49501/54209...\n",
      "Processing row 49601/54209...\n",
      "Processing row 49701/54209...\n",
      "Processing row 49801/54209...\n",
      "Processing row 49901/54209...\n",
      "Processing row 50001/54209...\n",
      "Processing row 50101/54209...\n",
      "Processing row 50201/54209...\n",
      "Processing row 50301/54209...\n",
      "Processing row 50401/54209...\n",
      "Processing row 50501/54209...\n",
      "Processing row 50601/54209...\n",
      "Processing row 50701/54209...\n",
      "Processing row 50801/54209...\n",
      "Processing row 50901/54209...\n",
      "Processing row 51001/54209...\n",
      "Processing row 51101/54209...\n",
      "Processing row 51201/54209...\n",
      "Processing row 51301/54209...\n",
      "Processing row 51401/54209...\n",
      "Processing row 51501/54209...\n",
      "Processing row 51601/54209...\n",
      "Processing row 51701/54209...\n",
      "Processing row 51801/54209...\n",
      "Processing row 51901/54209...\n",
      "Processing row 52001/54209...\n",
      "Processing row 52101/54209...\n",
      "Processing row 52201/54209...\n",
      "Processing row 52301/54209...\n",
      "Processing row 52401/54209...\n",
      "Processing row 52501/54209...\n",
      "Processing row 52601/54209...\n",
      "Processing row 52701/54209...\n",
      "Processing row 52801/54209...\n",
      "Processing row 52901/54209...\n",
      "Processing row 53001/54209...\n",
      "Processing row 53101/54209...\n",
      "Processing row 53201/54209...\n",
      "Processing row 53301/54209...\n",
      "Processing row 53401/54209...\n",
      "Processing row 53501/54209...\n",
      "Processing row 53601/54209...\n",
      "Processing row 53701/54209...\n",
      "Processing row 53801/54209...\n",
      "Processing row 53901/54209...\n",
      "Processing row 54001/54209...\n",
      "Processing row 54101/54209...\n",
      "Processing row 54201/54209...\n",
      "\n",
      "Processing complete!\n",
      "Results saved to: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/daily_raw_material_requirements_01.csv\n",
      "Total unique dates: 1357\n",
      "Total unique raw materials: 124\n",
      "Total rows in output: 89657\n"
     ]
    }
   ],
   "source": [
    "# Execute the raw materials processor\n",
    "base_path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "sales_path = os.path.join(base_path, 'data', 'processed', 'sales_data_cleaned.csv')\n",
    "menu_bom_path = os.path.join(base_path, 'data', 'raw', 'bom', 'menu_bom.csv')\n",
    "condiment_bom_path = os.path.join(base_path, 'data', 'raw', 'bom', 'condiment_bom.csv')\n",
    "output_path = os.path.join(base_path, 'data', 'processed', 'daily_raw_material_requirements_01.csv')\n",
    "\n",
    "# Initialize processor\n",
    "processor = RawMaterialProcessor(sales_path, menu_bom_path, condiment_bom_path)\n",
    "\n",
    "# Process and save\n",
    "processor.save_results(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully executed the complete data preprocessing pipeline:\n",
    "\n",
    "1. **Sales Data Merger**: Combined Indonesian and English sales data into a single standardized dataset\n",
    "2. **Sales Data Cleaner**: Standardized item names, expanded package deals, and removed discontinued items\n",
    "3. **Raw Materials Preprocessor**: Calculated daily raw material requirements based on sales data and BOM\n",
    "\n",
    "The processed data is now ready for further analysis and forecasting model development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
