{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation - Item Sales Aggregation\n",
    "\n",
    "This notebook contains the data preprocessing pipeline for the cafe supply forecasting project. It includes:\n",
    "\n",
    "1. Sales Data Merger - Merges sales data from different language sources\n",
    "2. Sales Data Cleaner - Cleans and standardizes sales data\n",
    "3. Item Sales Aggregator - Aggregates daily sales quantities for each item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import Dict, Optional, List, Set, Tuple\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sales Data Merger\n",
    "\n",
    "This section merges two sales CSV files with different languages (Indonesian and English) into a single standardized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_indonesian_to_english(df_indonesian: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Translate Indonesian column names and values to English\n",
    "\n",
    "    Args:\n",
    "        df_indonesian: DataFrame with Indonesian column names and values\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with translated English columns and values\n",
    "    \"\"\"\n",
    "    # Column mapping from Indonesian to English\n",
    "    column_mapping = {\n",
    "        'Tanggal': 'Date',\n",
    "        'Nomor struk': 'Receipt number',\n",
    "        'Jenis struk': 'Receipt type',\n",
    "        'Kategori': 'Category',\n",
    "        'SKU': 'SKU',\n",
    "        'Barang': 'Item',\n",
    "        'Varian': 'Variant',\n",
    "        'Pemodifikasi diterapkan': 'Modifiers applied',\n",
    "        'Kuantitas': 'Quantity',\n",
    "        'Penjualan Kotor': 'Gross sales',\n",
    "        'Diskon': 'Discounts',\n",
    "        'Penjualan bersih': 'Net sales',\n",
    "        'Harga pokok': 'Cost of goods',\n",
    "        'Laba kotor': 'Gross profit',\n",
    "        'Pajak': 'Taxes',\n",
    "        'Jenis pesanan': 'Dining option',\n",
    "        'POS': 'POS',\n",
    "        'Toko': 'Store',\n",
    "        'Nama Kasir': 'Cashier name',\n",
    "        'Nama Pelanggan': 'Customer name',\n",
    "        'Kontak Pelanggan': 'Customer contacts',\n",
    "        'Komentar': 'Comment',\n",
    "        'Keadaan': 'Status'\n",
    "    }\n",
    "    \n",
    "    # Rename columns\n",
    "    df_indonesian = df_indonesian.rename(columns=column_mapping)\n",
    "    \n",
    "    # Translate specific values\n",
    "    value_mappings = {\n",
    "        'Receipt type': {\n",
    "            'Penjualan': 'Sale'\n",
    "        },\n",
    "        'Dining option': {\n",
    "            'Makan di tempat': 'Dine in'\n",
    "        },\n",
    "        'Status': {\n",
    "            'Ditutup': 'Closed'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Apply value translations\n",
    "    for column, mapping in value_mappings.items():\n",
    "        if column in df_indonesian.columns:\n",
    "            df_indonesian[column] = df_indonesian[column].replace(mapping)\n",
    "    \n",
    "    return df_indonesian\n",
    "\n",
    "def parse_date(date_str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Parse different date formats to standard datetime\n",
    "\n",
    "    Args:\n",
    "        date_str: Date string in various formats\n",
    "\n",
    "    Returns:\n",
    "        Parsed datetime object or NaT if parsing fails\n",
    "    \"\"\"\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "\n",
    "    date_str = str(date_str).strip()\n",
    "\n",
    "    # Try different date formats\n",
    "    formats_to_try = [\n",
    "        '%d/%m/%y %H.%M',  # Indonesian format: 13/05/25 23.43\n",
    "        '%d/%m/%y %H:%M',  # Alternative Indonesian format\n",
    "        '%m/%d/%y %I:%M %p',  # English format: 9/25/25 10:07 PM\n",
    "        '%m/%d/%Y %I:%M %p',  # English format with 4-digit year\n",
    "    ]\n",
    "\n",
    "    for fmt in formats_to_try:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # If none of the specific formats work, try pandas auto-detection\n",
    "    try:\n",
    "        return pd.to_datetime(date_str)\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def clean_numeric_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean and convert numeric columns to proper numeric types\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with numeric columns that may contain non-numeric characters\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with cleaned numeric columns\n",
    "    \"\"\"\n",
    "    numeric_columns = [\n",
    "        'Quantity', 'Gross sales', 'Discounts', 'Net sales',\n",
    "        'Cost of goods', 'Gross profit', 'Taxes'\n",
    "    ]\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            # Remove any non-numeric characters except decimal point\n",
    "            df[col] = df[col].astype(str).str.replace(r'[^\\d.]', '', regex=True)\n",
    "            # Convert to numeric, errors='coerce' will turn invalid values to NaN\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_sales_files(file1_path: str, file2_path: str, output_path: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Merge two sales CSV files with different languages\n",
    "\n",
    "    Args:\n",
    "        file1_path: Path to Indonesian sales CSV file\n",
    "        file2_path: Path to English sales CSV file\n",
    "        output_path: Path where merged CSV should be saved\n",
    "\n",
    "    Returns:\n",
    "        Merged DataFrame or None if merge fails\n",
    "    \"\"\"\n",
    "    print(f'Reading file 1: {file1_path}')\n",
    "    print(f'Reading file 2: {file2_path}')\n",
    "\n",
    "    # Read both files\n",
    "    try:\n",
    "        # Indonesian file (first file)\n",
    "        df1 = pd.read_csv(file1_path, sep=';')\n",
    "        print(f'File 1 loaded successfully. Shape: {df1.shape}')\n",
    "\n",
    "        # English file (second file)\n",
    "        df2 = pd.read_csv(file2_path, sep=',')\n",
    "        print(f'File 2 loaded successfully. Shape: {df2.shape}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error reading files: {e}')\n",
    "        return None\n",
    "\n",
    "    # Translate Indonesian file to English\n",
    "    print('Translating Indonesian file to English...')\n",
    "    df1_translated = translate_indonesian_to_english(df1)\n",
    "\n",
    "    # Clean numeric columns in both dataframes\n",
    "    print('Cleaning numeric columns...')\n",
    "    df1_translated = clean_numeric_columns(df1_translated)\n",
    "    df2 = clean_numeric_columns(df2)\n",
    "\n",
    "    # Parse dates in both dataframes\n",
    "    print('Parsing dates...')\n",
    "    df1_translated['Date'] = df1_translated['Date'].apply(parse_date)\n",
    "    df2['Date'] = df2['Date'].apply(parse_date)\n",
    "    \n",
    "    # Ensure both dataframes have the same columns\n",
    "    all_columns = set(df1_translated.columns) | set(df2.columns)\n",
    "    \n",
    "    for col in all_columns:\n",
    "        if col not in df1_translated.columns:\n",
    "            df1_translated[col] = np.nan\n",
    "        if col not in df2.columns:\n",
    "            df2[col] = np.nan\n",
    "    \n",
    "    # Reorder columns to match English file order\n",
    "    column_order = [\n",
    "        'Date', 'Receipt number', 'Receipt type', 'Category', 'SKU', 'Item', \n",
    "        'Variant', 'Modifiers applied', 'Quantity', 'Gross sales', 'Discounts', \n",
    "        'Net sales', 'Cost of goods', 'Gross profit', 'Taxes', 'Dining option', \n",
    "        'POS', 'Store', 'Cashier name', 'Customer name', 'Customer contacts', \n",
    "        'Comment', 'Status'\n",
    "    ]\n",
    "    \n",
    "    df1_translated = df1_translated[column_order]\n",
    "    df2 = df2[column_order]\n",
    "    \n",
    "    # Combine the dataframes\n",
    "    print('Combining dataframes...')\n",
    "    combined_df = pd.concat([df1_translated, df2], ignore_index=True)\n",
    "\n",
    "    # Sort by date\n",
    "    print('Sorting by date...')\n",
    "    combined_df = combined_df.sort_values('Date', na_position='last')\n",
    "\n",
    "    # Reset index\n",
    "    combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "    # Save the merged file\n",
    "    print(f'Saving merged file to: {output_path}')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f'Merge completed successfully!')\n",
    "    print(f'Total records: {len(combined_df)}')\n",
    "    print(f'Date range: {combined_df[\"Date\"].min()} to {combined_df[\"Date\"].max()}')\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/raw/sales/receipts-by-item-2022-01-01-2025-06-30.csv\n",
      "Reading file 2: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/raw/sales/receipts-by-item-2025-05-01-2025-09-25.csv\n",
      "File 1 loaded successfully. Shape: (48588, 23)\n",
      "File 2 loaded successfully. Shape: (10189, 23)\n",
      "Translating Indonesian file to English...\n",
      "Cleaning numeric columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dw/hqyrj5817x1bnh5y864bj0gh0000gp/T/ipykernel_7566/3424459581.py:138: DtypeWarning: Columns (19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(file1_path, sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing dates...\n",
      "Combining dataframes...\n",
      "Sorting by date...\n",
      "Saving merged file to: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data.csv\n",
      "Merge completed successfully!\n",
      "Total records: 58777\n",
      "Date range: 2022-01-01 14:31:00 to 2025-09-25 22:07:00\n",
      "\n",
      "First few rows of merged data:\n",
      "                 Date Receipt number Receipt type Category    SKU  \\\n",
      "0 2022-01-01 14:31:00         Jan-82         Sale      Tea     41   \n",
      "1 2022-01-01 15:34:00         Jan-83         Sale      NaN  10044   \n",
      "2 2022-01-01 15:34:00         Jan-83         Sale      NaN  10049   \n",
      "3 2022-01-01 16:16:00         Jan-84         Sale    snack     84   \n",
      "4 2022-01-01 16:22:00         Jan-85         Sale    Other   3300   \n",
      "\n",
      "                Item  Variant  Modifiers applied  Quantity  Gross sales  ...  \\\n",
      "0      Hot Tea Ajeng      NaN                NaN       1.0      15000.0  ...   \n",
      "1    Rose pink latte      NaN                NaN       1.0      23000.0  ...   \n",
      "2  Roti bakar klasik      NaN                NaN       1.0      10000.0  ...   \n",
      "3     Kentang goreng      NaN                NaN       1.0      15000.0  ...   \n",
      "4          Spaghetti      NaN                NaN       1.0      25000.0  ...   \n",
      "\n",
      "   Gross profit  Taxes  Dining option    POS             Store Cashier name  \\\n",
      "0       13600.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "1       18000.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "2           NaN    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "3       12000.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "4       21000.0    0.0        Dine in  POS 1  Husgendam Coffee      Pemilik   \n",
      "\n",
      "  Customer name Customer contacts Comment  Status  \n",
      "0           NaN               NaN     NaN  Closed  \n",
      "1           NaN               NaN     NaN  Closed  \n",
      "2           NaN               NaN     NaN  Closed  \n",
      "3           NaN               NaN     NaN  Closed  \n",
      "4           NaN               NaN     NaN  Closed  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Data summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 58777 entries, 0 to 58776\n",
      "Data columns (total 23 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   Date               58777 non-null  datetime64[ns]\n",
      " 1   Receipt number     58777 non-null  object        \n",
      " 2   Receipt type       58777 non-null  object        \n",
      " 3   Category           52565 non-null  object        \n",
      " 4   SKU                58777 non-null  int64         \n",
      " 5   Item               58777 non-null  object        \n",
      " 6   Variant            0 non-null      float64       \n",
      " 7   Modifiers applied  0 non-null      float64       \n",
      " 8   Quantity           58777 non-null  float64       \n",
      " 9   Gross sales        57419 non-null  float64       \n",
      " 10  Discounts          58750 non-null  float64       \n",
      " 11  Net sales          57409 non-null  float64       \n",
      " 12  Cost of goods      16029 non-null  float64       \n",
      " 13  Gross profit       51141 non-null  float64       \n",
      " 14  Taxes              58777 non-null  float64       \n",
      " 15  Dining option      58763 non-null  object        \n",
      " 16  POS                58777 non-null  object        \n",
      " 17  Store              58777 non-null  object        \n",
      " 18  Cashier name       58777 non-null  object        \n",
      " 19  Customer name      1918 non-null   object        \n",
      " 20  Customer contacts  1876 non-null   object        \n",
      " 21  Comment            986 non-null    object        \n",
      " 22  Status             58777 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(12)\n",
      "memory usage: 10.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Execute the sales data merger\n",
    "base_path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "file1_path = os.path.join(base_path, 'data', 'raw', 'sales', 'receipts-by-item-2022-01-01-2025-06-30.csv')\n",
    "file2_path = os.path.join(base_path, 'data', 'raw', 'sales', 'receipts-by-item-2025-05-01-2025-09-25.csv')\n",
    "output_path = os.path.join(base_path, 'data', 'processed', 'sales_data.csv')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Merge the files\n",
    "merged_data = merge_sales_files(file1_path, file2_path, output_path)\n",
    "\n",
    "if merged_data is not None:\n",
    "    print('\\nFirst few rows of merged data:')\n",
    "    print(merged_data.head())\n",
    "\n",
    "    print('\\nData summary:')\n",
    "    print(merged_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sales Data Cleaner\n",
    "\n",
    "This section cleans and standardizes sales data to match the menu BOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesDataCleaner:\n",
    "    \"\"\"Clean and standardize sales data based on menu BOM\"\"\"\n",
    "\n",
    "    def __init__(self, sales_path: str, menu_bom_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the cleaner with file paths\n",
    "\n",
    "        Args:\n",
    "            sales_path: Path to sales data CSV\n",
    "            menu_bom_path: Path to menu BOM CSV\n",
    "        \"\"\"\n",
    "        self.sales_path = sales_path\n",
    "        self.menu_bom_path = menu_bom_path\n",
    "        self.sales_df = pd.read_csv(sales_path)\n",
    "        self.menu_bom_df = pd.read_csv(menu_bom_path)\n",
    "\n",
    "        # Build mappings\n",
    "        self.rename_map = self._build_rename_mapping()\n",
    "        self.package_items = self._build_package_mapping()\n",
    "        self.active_items = self._get_active_items()\n",
    "\n",
    "        # Track statistics\n",
    "        self.stats = {\n",
    "            'original_records': len(self.sales_df),\n",
    "            'renamed_records': 0,\n",
    "            'expanded_packages': 0,\n",
    "            'expanded_items': 0,\n",
    "            'discontinued_items': 0,\n",
    "            'removed_records': 0\n",
    "        }\n",
    "\n",
    "    def _build_rename_mapping(self) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Build mapping of old names to standardized names\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping old item names to new standardized names\n",
    "        \"\"\"\n",
    "        # Define rename mappings (sales name → BOM name)\n",
    "        rename_map = {\n",
    "            # Tea items\n",
    "            'lemon tea hot': 'Lemon Hot',\n",
    "            'lemon tea ice': 'Lemon Ice',\n",
    "            'hot tea ajeng': 'Ajeng Hot',\n",
    "            'ice tea ajeng': 'Ajeng Ice',\n",
    "            'hot tea anindya': 'Anindya Hot',\n",
    "            'ice tea anindya': 'Anindya Ice',\n",
    "\n",
    "            # Coffee items\n",
    "            'kopi susu panas': 'Kopi Susu Hot',\n",
    "            'kopi susu ice': 'Kopi Susu Ice',\n",
    "            'ice cappucino': 'Cappucino Ice',\n",
    "            'latte': 'Latte Hot',\n",
    "            'ice latte': 'Latte Ice',\n",
    "            'picolo': 'Piccolo',\n",
    "            'long black hot': 'Black Hot',\n",
    "            'long black ice': 'Black Ice',\n",
    "\n",
    "            # Milk-based items\n",
    "            'red velvet': 'Red Velvet Ice',\n",
    "            'susu coklat': 'Coklat Hot',\n",
    "\n",
    "            # V60 variants (all consolidate to v60)\n",
    "            'v60 hacienda natural': 'v60',\n",
    "            'v60 argopuro': 'v60',\n",
    "            'v60 finca': 'v60',\n",
    "\n",
    "            # Noodle items\n",
    "            'mie goreng telur': 'Mie Goreng',\n",
    "            'mie rebus telur': 'Mie Rebus',\n",
    "\n",
    "            # Rice items\n",
    "            'nasi goreng djawa': 'Nasi Goreng Jawa',\n",
    "            'nasi ayam daun jeruk': 'Ayam Daun Jeruk',\n",
    "            'nasi ayam rempah': 'Ayam Rempah',\n",
    "\n",
    "            # Main course items\n",
    "            'ayam mentega (paket)': 'Ayam Mentega',\n",
    "            'ayam curry (paket)': 'Ayam Curry',\n",
    "            'ayam dauh jeruk (paket)': 'Ayam Daun Jeruk',\n",
    "\n",
    "            # Snacks\n",
    "            'cireng isi': 'Cireng',\n",
    "\n",
    "            # Desserts\n",
    "            'pannacotta': 'Panacotta',\n",
    "\n",
    "            # Additional items\n",
    "            'add vanilla ice cream': 'Add Ice Cream Vanilla',\n",
    "            'add telur ceplok': 'Add Telur',\n",
    "            'add telur dadar': 'Add Telur',\n",
    "            'add caramel topping': 'Add Topping Caramel',\n",
    "\n",
    "            # Case variations for consistency\n",
    "            'kentang goreng': 'Kentang Goreng',\n",
    "            'lychee tea hot': 'Lychee Tea Hot',\n",
    "            'lychee tea ice': 'Lychee Tea Ice',\n",
    "            'matcha ice': 'Matcha Ice',\n",
    "            'mie goreng': 'Mie Goreng',\n",
    "            'mie rebus': 'Mie Rebus',\n",
    "            'nasi gila': 'Nasi Gila',\n",
    "            'red velvet ice': 'Red Velvet Ice',\n",
    "            'roti bakar coklat': 'Roti Bakar Coklat',\n",
    "            'roti bakar klasik': 'Roti Bakar Klasik',\n",
    "            'roti bakar strawberry': 'Roti Bakar Strawberry',\n",
    "            'singkong keju': 'Singkong Keju',\n",
    "            'tahu tuna': 'Tahu Tuna',\n",
    "            'taro ice': 'Taro Ice',\n",
    "            'tempe mendoan': 'Tempe Mendoan',\n",
    "            'pisang goreng aren': 'Pisang Goreng Aren',\n",
    "            'vietnam drip': 'Vietnam Drip',\n",
    "            'waffle vanilla': 'Waffle Vanilla',\n",
    "            'add keju': 'Add Keju',\n",
    "            'add kerupuk': 'Add Kerupuk',\n",
    "            'coklat ice': 'Coklat Ice',\n",
    "            'cheese cake': 'Cheese Cake',\n",
    "        }\n",
    "\n",
    "        return rename_map\n",
    "\n",
    "    def _build_package_mapping(self) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"\n",
    "        Build mapping for package deals that need to be split\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping package names to list of (item, quantity multiplier) tuples\n",
    "        \"\"\"\n",
    "        package_map = {\n",
    "            # Mie with telur - add telur as separate item\n",
    "            'mie goreng telur': [\n",
    "                ('Mie Goreng', 1.0),\n",
    "                ('Add Telur', 1.0)\n",
    "            ],\n",
    "            'mie rebus telur': [\n",
    "                ('Mie Rebus', 1.0),\n",
    "                ('Add Telur', 1.0)\n",
    "            ],\n",
    "            # Package deals\n",
    "            'paket ayam teriyaki + lemon tea': [\n",
    "                ('Ayam Teriyaki', 1.0),\n",
    "                ('Lemon Ice', 1.0)\n",
    "            ],\n",
    "            'paket ayam curry + lemon tea': [\n",
    "                ('Ayam Curry', 1.0),\n",
    "                ('Lemon Ice', 1.0)\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        return package_map\n",
    "\n",
    "    def _get_active_items(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get set of active items from menu BOM\n",
    "\n",
    "        Returns:\n",
    "            Set of normalized active item names\n",
    "        \"\"\"\n",
    "        active_items = set()\n",
    "\n",
    "        # Get unique items from menu BOM\n",
    "        for item_name in self.menu_bom_df['Item'].unique():\n",
    "            # Normalize by stripping whitespace and converting to lowercase\n",
    "            normalized = item_name.strip().lower()\n",
    "            active_items.add(normalized)\n",
    "\n",
    "        return active_items\n",
    "\n",
    "    def _normalize_item_name(self, item_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize item name for comparison\n",
    "\n",
    "        Args:\n",
    "            item_name: Original item name\n",
    "\n",
    "        Returns:\n",
    "            Normalized item name (lowercase, stripped)\n",
    "        \"\"\"\n",
    "        if pd.isna(item_name):\n",
    "            return ''\n",
    "        return str(item_name).strip().lower()\n",
    "\n",
    "    def standardize_names(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Standardize item names in sales data\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to standardize\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with standardized item names\n",
    "        \"\"\"\n",
    "        print('\\n' + '-' * 80)\n",
    "        print('STEP 1: STANDARDIZING ITEM NAMES')\n",
    "        print('-' * 80)\n",
    "\n",
    "        # Create a copy to work with\n",
    "        standardized_df = df.copy()\n",
    "\n",
    "        # Apply simple renames first\n",
    "        renamed_count = 0\n",
    "        for old_name, new_name in self.rename_map.items():\n",
    "            # Case-insensitive matching\n",
    "            mask = standardized_df['Item'].str.lower() == old_name.lower()\n",
    "            if mask.any():\n",
    "                count = mask.sum()\n",
    "                standardized_df.loc[mask, 'Item'] = new_name\n",
    "                renamed_count += count\n",
    "                print(f'  ✓ Renamed: \"{old_name}\" → \"{new_name}\" ({count} records)')\n",
    "\n",
    "        self.stats['renamed_records'] = renamed_count\n",
    "\n",
    "        # Handle package items that need to be split\n",
    "        expanded_rows = []\n",
    "        rows_to_remove = []\n",
    "\n",
    "        for idx, row in standardized_df.iterrows():\n",
    "            item_lower = str(row['Item']).lower()\n",
    "\n",
    "            if item_lower in self.package_items:\n",
    "                # Mark this row for removal\n",
    "                rows_to_remove.append(idx)\n",
    "\n",
    "                # Create new rows for each component\n",
    "                components = self.package_items[item_lower]\n",
    "                original_qty = row['Quantity']\n",
    "\n",
    "                for component_item, qty_multiplier in components:\n",
    "                    new_row = row.copy()\n",
    "                    new_row['Item'] = component_item\n",
    "                    new_row['Quantity'] = original_qty * qty_multiplier\n",
    "                    expanded_rows.append(new_row)\n",
    "\n",
    "                print(f'  ✓ Expanded: \"{row[\"Item\"]}\" → {len(components)} items ({original_qty} qty each)')\n",
    "\n",
    "        # Remove package items\n",
    "        if rows_to_remove:\n",
    "            standardized_df = standardized_df.drop(rows_to_remove)\n",
    "            self.stats['expanded_packages'] = len(rows_to_remove)\n",
    "\n",
    "        # Add expanded rows\n",
    "        if expanded_rows:\n",
    "            expanded_df = pd.DataFrame(expanded_rows)\n",
    "            standardized_df = pd.concat([standardized_df, expanded_df], ignore_index=True)\n",
    "            self.stats['expanded_items'] = len(expanded_rows)\n",
    "\n",
    "        # Sort by date\n",
    "        standardized_df = standardized_df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "        print(f'\\nStandardization Summary:')\n",
    "        print(f'  Simple renames: {renamed_count} records')\n",
    "        print(f'  Package expansions: {len(rows_to_remove)} packages → {len(expanded_rows)} items')\n",
    "        print(f'  Total records: {len(df)} → {len(standardized_df)}')\n",
    "\n",
    "        return standardized_df\n",
    "\n",
    "    def identify_discontinued_items(self, df: pd.DataFrame) -> Tuple[Set[str], pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Identify items in sales data that are not in menu BOM\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to check\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (set of discontinued item names, DataFrame with discontinued items stats, DataFrame with valid items)\n",
    "        \"\"\"\n",
    "        print('\\n' + '-' * 80)\n",
    "        print('STEP 2: IDENTIFYING DISCONTINUED ITEMS')\n",
    "        print('-' * 80)\n",
    "\n",
    "        # Get all unique items from sales data\n",
    "        sales_items = df['Item'].dropna().unique()\n",
    "\n",
    "        # Find discontinued items (in sales but not in menu BOM)\n",
    "        discontinued_items = set()\n",
    "        valid_items = set()\n",
    "\n",
    "        for item in sales_items:\n",
    "            normalized = self._normalize_item_name(item)\n",
    "            if normalized and normalized not in self.active_items:\n",
    "                discontinued_items.add(item)  # Store original name for display\n",
    "            else:\n",
    "                valid_items.add(item)\n",
    "\n",
    "        self.stats['discontinued_items'] = len(discontinued_items)\n",
    "\n",
    "        # Create statistics DataFrame for discontinued items\n",
    "        discontinued_stats = []\n",
    "        for item in discontinued_items:\n",
    "            item_data = df[df['Item'] == item]\n",
    "            stats = {\n",
    "                'Item': item,\n",
    "                'Total_Quantity_Sold': item_data['Quantity'].sum(),\n",
    "                'Total_Transactions': len(item_data),\n",
    "                'First_Sale_Date': item_data['Date'].min(),\n",
    "                'Last_Sale_Date': item_data['Date'].max()\n",
    "            }\n",
    "            discontinued_stats.append(stats)\n",
    "\n",
    "        discontinued_df = pd.DataFrame(discontinued_stats)\n",
    "\n",
    "        # Sort by total quantity sold (descending)\n",
    "        if not discontinued_df.empty:\n",
    "            discontinued_df = discontinued_df.sort_values('Total_Quantity_Sold', ascending=False)\n",
    "\n",
    "        print(f'\\n✓ Valid items (found in BOM): {len(valid_items)}')\n",
    "        print(f'✗ Discontinued items (NOT in BOM): {len(discontinued_items)}')\n",
    "\n",
    "        return discontinued_items, discontinued_df, df\n",
    "\n",
    "    def remove_discontinued_items(self, df: pd.DataFrame, discontinued_items: Set[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove discontinued items from sales data\n",
    "\n",
    "        Args:\n",
    "            df: DataFrame to clean\n",
    "            discontinued_items: Set of discontinued item names to remove\n",
    "\n",
    "        Returns:\n",
    "            Cleaned DataFrame with discontinued items removed\n",
    "        \"\"\"\n",
    "        # Filter out discontinued items\n",
    "        cleaned_df = df[~df['Item'].isin(discontinued_items)].copy()\n",
    "\n",
    "        self.stats['removed_records'] = len(df) - len(cleaned_df)\n",
    "\n",
    "        return cleaned_df\n",
    "\n",
    "    def save_cleaned_data(self, cleaned_df: pd.DataFrame, output_path: str):\n",
    "        \"\"\"\n",
    "        Save cleaned sales data to CSV\n",
    "\n",
    "        Args:\n",
    "            cleaned_df: Cleaned DataFrame\n",
    "            output_path: Path where cleaned CSV should be saved\n",
    "        \"\"\"\n",
    "        # Create output directory if needed\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        cleaned_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f'\\n✓ Cleaned data saved to: {output_path}')\n",
    "        print(f'  Total records: {len(cleaned_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SALES DATA CLEANER\n",
      "================================================================================\n",
      "\n",
      "Sales data: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data.csv\n",
      "Menu BOM: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/raw/bom/menu_bom.csv\n",
      "Output: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data_cleaned.csv\n",
      "\n",
      "Original records: 58,777\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 1: STANDARDIZING ITEM NAMES\n",
      "--------------------------------------------------------------------------------\n",
      "  ✓ Renamed: \"lemon tea hot\" → \"Lemon Hot\" (1004 records)\n",
      "  ✓ Renamed: \"lemon tea ice\" → \"Lemon Ice\" (984 records)\n",
      "  ✓ Renamed: \"hot tea ajeng\" → \"Ajeng Hot\" (499 records)\n",
      "  ✓ Renamed: \"ice tea ajeng\" → \"Ajeng Ice\" (226 records)\n",
      "  ✓ Renamed: \"hot tea anindya\" → \"Anindya Hot\" (207 records)\n",
      "  ✓ Renamed: \"ice tea anindya\" → \"Anindya Ice\" (112 records)\n",
      "  ✓ Renamed: \"kopi susu panas\" → \"Kopi Susu Hot\" (715 records)\n",
      "  ✓ Renamed: \"kopi susu ice\" → \"Kopi Susu Ice\" (3727 records)\n",
      "  ✓ Renamed: \"ice cappucino\" → \"Cappucino Ice\" (264 records)\n",
      "  ✓ Renamed: \"latte\" → \"Latte Hot\" (242 records)\n",
      "  ✓ Renamed: \"ice latte\" → \"Latte Ice\" (120 records)\n",
      "  ✓ Renamed: \"picolo\" → \"Piccolo\" (206 records)\n",
      "  ✓ Renamed: \"long black hot\" → \"Black Hot\" (54 records)\n",
      "  ✓ Renamed: \"long black ice\" → \"Black Ice\" (50 records)\n",
      "  ✓ Renamed: \"red velvet\" → \"Red Velvet Ice\" (139 records)\n",
      "  ✓ Renamed: \"susu coklat\" → \"Coklat Hot\" (1 records)\n",
      "  ✓ Renamed: \"v60 hacienda natural\" → \"v60\" (220 records)\n",
      "  ✓ Renamed: \"v60 argopuro\" → \"v60\" (60 records)\n",
      "  ✓ Renamed: \"v60 finca\" → \"v60\" (45 records)\n",
      "  ✓ Renamed: \"mie goreng telur\" → \"Mie Goreng\" (764 records)\n",
      "  ✓ Renamed: \"mie rebus telur\" → \"Mie Rebus\" (324 records)\n",
      "  ✓ Renamed: \"nasi goreng djawa\" → \"Nasi Goreng Jawa\" (686 records)\n",
      "  ✓ Renamed: \"nasi ayam daun jeruk\" → \"Ayam Daun Jeruk\" (197 records)\n",
      "  ✓ Renamed: \"nasi ayam rempah\" → \"Ayam Rempah\" (44 records)\n",
      "  ✓ Renamed: \"ayam mentega (paket)\" → \"Ayam Mentega\" (34 records)\n",
      "  ✓ Renamed: \"ayam curry (paket)\" → \"Ayam Curry\" (36 records)\n",
      "  ✓ Renamed: \"cireng isi\" → \"Cireng\" (44 records)\n",
      "  ✓ Renamed: \"pannacotta\" → \"Panacotta\" (311 records)\n",
      "  ✓ Renamed: \"add vanilla ice cream\" → \"Add Ice Cream Vanilla\" (178 records)\n",
      "  ✓ Renamed: \"add telur ceplok\" → \"Add Telur\" (175 records)\n",
      "  ✓ Renamed: \"add telur dadar\" → \"Add Telur\" (41 records)\n",
      "  ✓ Renamed: \"add caramel topping\" → \"Add Topping Caramel\" (29 records)\n",
      "  ✓ Renamed: \"kentang goreng\" → \"Kentang Goreng\" (1994 records)\n",
      "  ✓ Renamed: \"lychee tea hot\" → \"Lychee Tea Hot\" (635 records)\n",
      "  ✓ Renamed: \"lychee tea ice\" → \"Lychee Tea Ice\" (2499 records)\n",
      "  ✓ Renamed: \"matcha ice\" → \"Matcha Ice\" (1029 records)\n",
      "  ✓ Renamed: \"mie goreng\" → \"Mie Goreng\" (1229 records)\n",
      "  ✓ Renamed: \"mie rebus\" → \"Mie Rebus\" (639 records)\n",
      "  ✓ Renamed: \"nasi gila\" → \"Nasi Gila\" (984 records)\n",
      "  ✓ Renamed: \"red velvet ice\" → \"Red Velvet Ice\" (668 records)\n",
      "  ✓ Renamed: \"roti bakar coklat\" → \"Roti Bakar Coklat\" (791 records)\n",
      "  ✓ Renamed: \"roti bakar klasik\" → \"Roti Bakar Klasik\" (288 records)\n",
      "  ✓ Renamed: \"roti bakar strawberry\" → \"Roti Bakar Strawberry\" (142 records)\n",
      "  ✓ Renamed: \"singkong keju\" → \"Singkong Keju\" (737 records)\n",
      "  ✓ Renamed: \"tahu tuna\" → \"Tahu Tuna\" (1085 records)\n",
      "  ✓ Renamed: \"taro ice\" → \"Taro Ice\" (480 records)\n",
      "  ✓ Renamed: \"tempe mendoan\" → \"Tempe Mendoan\" (487 records)\n",
      "  ✓ Renamed: \"pisang goreng aren\" → \"Pisang Goreng Aren\" (550 records)\n",
      "  ✓ Renamed: \"vietnam drip\" → \"Vietnam Drip\" (1849 records)\n",
      "  ✓ Renamed: \"waffle vanilla\" → \"Waffle Vanilla\" (516 records)\n",
      "  ✓ Renamed: \"add keju\" → \"Add Keju\" (110 records)\n",
      "  ✓ Renamed: \"add kerupuk\" → \"Add Kerupuk\" (63 records)\n",
      "  ✓ Renamed: \"coklat ice\" → \"Coklat Ice\" (945 records)\n",
      "  ✓ Renamed: \"cheese cake\" → \"Cheese Cake\" (101 records)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (4.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (13.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (3.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (3.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (6.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (6.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (2.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (7.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Curry + Lemon Tea\" → 2 items (1.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (20.0 qty each)\n",
      "  ✓ Expanded: \"Paket Ayam Teriyaki + Lemon Tea\" → 2 items (3.0 qty each)\n",
      "\n",
      "Standardization Summary:\n",
      "  Simple renames: 29559 records\n",
      "  Package expansions: 36 packages → 72 items\n",
      "  Total records: 58777 → 58813\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 2: IDENTIFYING DISCONTINUED ITEMS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Valid items (found in BOM): 87\n",
      "✗ Discontinued items (NOT in BOM): 138\n",
      "\n",
      "Total discontinued items: 138\n",
      "Total transactions affected: 4604\n",
      "Total quantity sold: 5361\n",
      "\n",
      "Top 10 discontinued items:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item</th>\n",
       "      <th>Total_Quantity_Sold</th>\n",
       "      <th>Total_Transactions</th>\n",
       "      <th>First_Sale_Date</th>\n",
       "      <th>Last_Sale_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Americano Ice</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>981</td>\n",
       "      <td>2022-01-05 17:05:00</td>\n",
       "      <td>2025-05-13 18:08:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Air mineral</td>\n",
       "      <td>655.0</td>\n",
       "      <td>440</td>\n",
       "      <td>2022-01-25 14:54:00</td>\n",
       "      <td>2025-05-12 22:27:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Americano Hot</td>\n",
       "      <td>399.0</td>\n",
       "      <td>376</td>\n",
       "      <td>2022-01-10 18:49:00</td>\n",
       "      <td>2025-05-13 23:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Ayam suwir</td>\n",
       "      <td>365.0</td>\n",
       "      <td>342</td>\n",
       "      <td>2022-10-19 16:41:00</td>\n",
       "      <td>2025-05-12 20:01:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Rose Tea Hot</td>\n",
       "      <td>283.0</td>\n",
       "      <td>277</td>\n",
       "      <td>2022-08-17 11:54:00</td>\n",
       "      <td>2025-04-17 23:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Air Mineral</td>\n",
       "      <td>261.0</td>\n",
       "      <td>235</td>\n",
       "      <td>2023-01-06 18:20:00</td>\n",
       "      <td>2025-09-24 14:43:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Nasi</td>\n",
       "      <td>175.0</td>\n",
       "      <td>148</td>\n",
       "      <td>2022-01-03 20:48:00</td>\n",
       "      <td>2025-05-12 17:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Air Mineral besar</td>\n",
       "      <td>165.0</td>\n",
       "      <td>142</td>\n",
       "      <td>2024-12-14 17:29:00</td>\n",
       "      <td>2025-05-12 19:44:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>PANNACOTTA 50 cup</td>\n",
       "      <td>160.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2023-06-28 16:15:00</td>\n",
       "      <td>2023-07-14 14:38:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Rose Tea Ice</td>\n",
       "      <td>132.0</td>\n",
       "      <td>129</td>\n",
       "      <td>2024-05-01 14:08:00</td>\n",
       "      <td>2025-04-15 21:49:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Item  Total_Quantity_Sold  Total_Transactions  \\\n",
       "110      Americano Ice               1003.0                 981   \n",
       "129        Air mineral                655.0                 440   \n",
       "111      Americano Hot                399.0                 376   \n",
       "112         Ayam suwir                365.0                 342   \n",
       "114       Rose Tea Hot                283.0                 277   \n",
       "71         Air Mineral                261.0                 235   \n",
       "76                Nasi                175.0                 148   \n",
       "69   Air Mineral besar                165.0                 142   \n",
       "109  PANNACOTTA 50 cup                160.0                   3   \n",
       "59        Rose Tea Ice                132.0                 129   \n",
       "\n",
       "         First_Sale_Date       Last_Sale_Date  \n",
       "110  2022-01-05 17:05:00  2025-05-13 18:08:00  \n",
       "129  2022-01-25 14:54:00  2025-05-12 22:27:00  \n",
       "111  2022-01-10 18:49:00  2025-05-13 23:43:00  \n",
       "112  2022-10-19 16:41:00  2025-05-12 20:01:00  \n",
       "114  2022-08-17 11:54:00  2025-04-17 23:12:00  \n",
       "71   2023-01-06 18:20:00  2025-09-24 14:43:00  \n",
       "76   2022-01-03 20:48:00  2025-05-12 17:45:00  \n",
       "69   2024-12-14 17:29:00  2025-05-12 19:44:00  \n",
       "109  2023-06-28 16:15:00  2023-07-14 14:38:00  \n",
       "59   2024-05-01 14:08:00  2025-04-15 21:49:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 3: HANDLING DISCONTINUED ITEMS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Removing discontinued items...\n",
      "✓ Removed 4,604 records\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "STEP 4: SAVING CLEANED DATA\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ Cleaned data saved to: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/sales_data_cleaned.csv\n",
      "  Total records: 54209\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Original records: 58,777\n",
      "\n",
      "Standardization:\n",
      "  - Renamed records: 29,559\n",
      "  - Expanded packages: 36 → 72 items\n",
      "\n",
      "Discontinued items:\n",
      "  - Items found: 138\n",
      "  - Records removed: 4,604\n",
      "\n",
      "Final records: 54,245\n",
      "================================================================================\n",
      "\n",
      "✓ Sales data cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Execute the sales data cleaner\n",
    "base_path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "sales_path = os.path.join(base_path, 'data', 'processed', 'sales_data.csv')\n",
    "menu_bom_path = os.path.join(base_path, 'data', 'raw', 'bom', 'menu_bom.csv')\n",
    "output_path = os.path.join(base_path, 'data', 'processed', 'sales_data_cleaned.csv')\n",
    "\n",
    "print('=' * 80)\n",
    "print('SALES DATA CLEANER')\n",
    "print('=' * 80)\n",
    "print(f'\\nSales data: {sales_path}')\n",
    "print(f'Menu BOM: {menu_bom_path}')\n",
    "print(f'Output: {output_path}')\n",
    "\n",
    "# Initialize cleaner\n",
    "cleaner = SalesDataCleaner(sales_path, menu_bom_path)\n",
    "\n",
    "print(f'\\nOriginal records: {len(cleaner.sales_df):,}')\n",
    "\n",
    "# Step 1: Standardize names\n",
    "standardized_df = cleaner.standardize_names(cleaner.sales_df)\n",
    "\n",
    "# Step 2: Identify discontinued items\n",
    "discontinued_items, discontinued_df, current_df = cleaner.identify_discontinued_items(standardized_df)\n",
    "\n",
    "# Print discontinued items report\n",
    "if discontinued_items:\n",
    "    print(f'\\nTotal discontinued items: {len(discontinued_items)}')\n",
    "    print(f'Total transactions affected: {discontinued_df[\"Total_Transactions\"].sum():.0f}')\n",
    "    print(f'Total quantity sold: {discontinued_df[\"Total_Quantity_Sold\"].sum():.0f}')\n",
    "    \n",
    "    # Display top discontinued items\n",
    "    print('\\nTop 10 discontinued items:')\n",
    "    display(discontinued_df.head(10))\n",
    "else:\n",
    "    print('\\n✓ No discontinued items found!')\n",
    "\n",
    "# Step 3: Handle discontinued items (removing them)\n",
    "print('\\n' + '-' * 80)\n",
    "print('STEP 3: HANDLING DISCONTINUED ITEMS')\n",
    "print('-' * 80)\n",
    "\n",
    "if discontinued_items:\n",
    "    print('\\nRemoving discontinued items...')\n",
    "    final_df = cleaner.remove_discontinued_items(current_df, discontinued_items)\n",
    "    print(f'✓ Removed {cleaner.stats[\"removed_records\"]:,} records')\n",
    "else:\n",
    "    print('\\n✓ No discontinued items to remove')\n",
    "    final_df = current_df\n",
    "\n",
    "# Save cleaned data\n",
    "print('\\n' + '-' * 80)\n",
    "print('STEP 4: SAVING CLEANED DATA')\n",
    "print('-' * 80)\n",
    "cleaner.save_cleaned_data(final_df, output_path)\n",
    "\n",
    "# Print final summary\n",
    "print('\\n' + '=' * 80)\n",
    "print('FINAL SUMMARY')\n",
    "print('=' * 80)\n",
    "\n",
    "print(f'\\nOriginal records: {cleaner.stats[\"original_records\"]:,}')\n",
    "print(f'\\nStandardization:')\n",
    "print(f'  - Renamed records: {cleaner.stats[\"renamed_records\"]:,}')\n",
    "print(f'  - Expanded packages: {cleaner.stats[\"expanded_packages\"]:,} → {cleaner.stats[\"expanded_items\"]:,} items')\n",
    "\n",
    "print(f'\\nDiscontinued items:')\n",
    "print(f'  - Items found: {cleaner.stats[\"discontinued_items\"]:,}')\n",
    "print(f'  - Records removed: {cleaner.stats[\"removed_records\"]:,}')\n",
    "\n",
    "final_records = cleaner.stats[\"original_records\"] + cleaner.stats[\"expanded_items\"] - cleaner.stats[\"removed_records\"]\n",
    "print(f'\\nFinal records: {final_records:,}')\n",
    "print('=' * 80)\n",
    "\n",
    "print('\\n✓ Sales data cleaning complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Item Sales Aggregator\n",
    "\n",
    "This section aggregates daily sales quantities for each item instead of calculating raw material requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemSalesAggregator:\n",
    "    \"\"\"Aggregate daily sales quantities for each item\"\"\"\n",
    "\n",
    "    def __init__(self, sales_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the aggregator with sales data path\n",
    "\n",
    "        Args:\n",
    "            sales_path: Path to cleaned sales data CSV\n",
    "        \"\"\"\n",
    "        self.sales_df = pd.read_csv(sales_path)\n",
    "\n",
    "    def aggregate_daily_sales(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate sales data to get daily quantities for each item\n",
    "\n",
    "        Returns:\n",
    "            DataFrame with columns: Date, Item, Quantity_Sold\n",
    "        \"\"\"\n",
    "        print('Starting item sales aggregation...')\n",
    "\n",
    "        # Make a copy to avoid modifying original\n",
    "        df = self.sales_df.copy()\n",
    "        \n",
    "        # Convert Date column to datetime and extract just the date\n",
    "        df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
    "        \n",
    "        # Clean Item names\n",
    "        df['Item'] = df['Item'].str.strip()\n",
    "        \n",
    "        # Consolidate espresso variants - treat 'espresso bon-bon' as 'espresso'\n",
    "        df['Item'] = df['Item'].str.replace(r'^espresso bon-bon$', 'espresso', case=False, regex=True)\n",
    "        \n",
    "        # Filter out items with 'add' prefix (case insensitive) - these are modifiers\n",
    "        initial_count = len(df)\n",
    "        df = df[~df['Item'].str.lower().str.startswith('add')]\n",
    "        filtered_count = initial_count - len(df)\n",
    "        if filtered_count > 0:\n",
    "            print(f\"Filtered out {filtered_count} transactions with 'add' prefix (modifiers)\")\n",
    "        \n",
    "        # Filter out discontinued items - cheese cake\n",
    "        initial_count = len(df)\n",
    "        df = df[~df['Item'].str.lower().str.contains('cheese cake')]\n",
    "        filtered_count = initial_count - len(df)\n",
    "        if filtered_count > 0:\n",
    "            print(f\"Filtered out {filtered_count} transactions for discontinued item 'cheese cake'\")\n",
    "\n",
    "        # Group by Date and Item, sum the Quantity\n",
    "        aggregated_df = df.groupby(['Date', 'Item'])['Quantity'].sum().reset_index()\n",
    "\n",
    "        # Rename columns for clarity\n",
    "        aggregated_df = aggregated_df.rename(columns={'Quantity': 'Quantity_Sold'})\n",
    "\n",
    "        # Sort by Date and Item\n",
    "        aggregated_df = aggregated_df.sort_values(['Date', 'Item']).reset_index(drop=True)\n",
    "\n",
    "        print(f'Aggregation complete!')\n",
    "        print(f'Total unique dates: {aggregated_df[\"Date\"].nunique()}')\n",
    "        print(f'Total unique items: {aggregated_df[\"Item\"].nunique()}')\n",
    "        print(f'Total rows in output: {len(aggregated_df)}')\n",
    "\n",
    "        return aggregated_df\n",
    "\n",
    "    def save_aggregated_data(self, output_path: str):\n",
    "        \"\"\"\n",
    "        Aggregate sales data and save results to CSV\n",
    "\n",
    "        Args:\n",
    "            output_path: Path where the output CSV should be saved\n",
    "        \"\"\"\n",
    "        result_df = self.aggregate_daily_sales()\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        # Save to CSV\n",
    "        result_df.to_csv(output_path, index=False)\n",
    "        print(f'\\nResults saved to: {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting item sales aggregation...\n",
      "Filtered out 1051 transactions with 'add' prefix (modifiers)\n",
      "Filtered out 101 transactions for discontinued item 'cheese cake'\n",
      "Aggregation complete!\n",
      "Total unique dates: 1357\n",
      "Total unique items: 78\n",
      "Total rows in output: 31563\n",
      "\n",
      "Results saved to: /Users/wyk/Documents/personal/thesis/cafe-supply-forecasting/data/processed/daily_item_sales.csv\n"
     ]
    }
   ],
   "source": [
    "# Execute the item sales aggregator\n",
    "base_path = os.path.dirname(os.path.dirname(os.path.abspath('__file__')))\n",
    "sales_path = os.path.join(base_path, 'data', 'processed', 'sales_data_cleaned.csv')\n",
    "output_path = os.path.join(base_path, 'data', 'processed', 'daily_item_sales.csv')\n",
    "\n",
    "# Initialize aggregator\n",
    "aggregator = ItemSalesAggregator(sales_path)\n",
    "\n",
    "# Aggregate and save\n",
    "aggregator.save_aggregated_data(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully executed the complete data preprocessing pipeline:\n",
    "\n",
    "1. **Sales Data Merger**: Combined Indonesian and English sales data into a single standardized dataset\n",
    "2. **Sales Data Cleaner**: Standardized item names, expanded package deals, and removed discontinued items\n",
    "3. **Item Sales Aggregator**: Aggregated daily sales quantities for each item\n",
    "\n",
    "The processed data is now ready for further analysis and forecasting model development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cafe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
